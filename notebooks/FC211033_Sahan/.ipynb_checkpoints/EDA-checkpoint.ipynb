{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06db754f",
   "metadata": {},
   "source": [
    "# Emotion Detection Dataset Analysis\n",
    "## Exploratory Data Analysis for Music Recommendation System\n",
    "\n",
    "This notebook performs comprehensive analysis of the FER2013 dataset to understand the data distribution, quality, and characteristics for building an emotion detection model for music recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4021e410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from collections import Counter, defaultdict\n",
    "import json\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'dataset_root': '../../data/raw/fer2013',\n",
    "    'train_dir': '../../data/raw/fer2013/train',\n",
    "    'test_dir': '../../data/raw/fer2013/test',\n",
    "    'output_dir': '../../data/processed/FC211033_Sahan',\n",
    "    'original_emotions': ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise'],\n",
    "    'music_relevant_emotions': ['angry', 'happy', 'neutral', 'sad', 'surprise'],\n",
    "    'excluded_emotions': ['disgust', 'fear']\n",
    "}\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(CONFIG['output_dir'], exist_ok=True)\n",
    "\n",
    "print(\"Emotion Detection Dataset Analysis Started\")\n",
    "print(f\"Dataset Location: {CONFIG['dataset_root']}\")\n",
    "print(f\"Output Directory: {CONFIG['output_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622f8201",
   "metadata": {},
   "source": [
    "## 1. Dataset Overview and Structure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa62839",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset_structure():\n",
    "    \"\"\"Analyze the structure and distribution of the dataset\"\"\"\n",
    "    \n",
    "    train_stats = {}\n",
    "    test_stats = {}\n",
    "    \n",
    "    print(\"Dataset Structure Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Analyze training data\n",
    "    if os.path.exists(CONFIG['train_dir']):\n",
    "        print(\"\\nTraining Data:\")\n",
    "        for emotion in CONFIG['original_emotions']:\n",
    "            emotion_path = os.path.join(CONFIG['train_dir'], emotion)\n",
    "            if os.path.exists(emotion_path):\n",
    "                count = len([f for f in os.listdir(emotion_path) \n",
    "                           if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "                train_stats[emotion] = count\n",
    "                relevance = \"✓\" if emotion in CONFIG['music_relevant_emotions'] else \"✗\"\n",
    "                print(f\"  {emotion:>8}: {count:>6} images {relevance}\")\n",
    "    \n",
    "    # Analyze test data\n",
    "    if os.path.exists(CONFIG['test_dir']):\n",
    "        print(\"\\nTest Data:\")\n",
    "        for emotion in CONFIG['original_emotions']:\n",
    "            emotion_path = os.path.join(CONFIG['test_dir'], emotion)\n",
    "            if os.path.exists(emotion_path):\n",
    "                count = len([f for f in os.listdir(emotion_path) \n",
    "                           if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "                test_stats[emotion] = count\n",
    "                relevance = \"✓\" if emotion in CONFIG['music_relevant_emotions'] else \"✗\"\n",
    "                print(f\"  {emotion:>8}: {count:>6} images {relevance}\")\n",
    "    \n",
    "    print(f\"\\nLegend: ✓ = Music-relevant emotion, ✗ = Excluded from final model\")\n",
    "    \n",
    "    return train_stats, test_stats\n",
    "\n",
    "train_distribution, test_distribution = analyze_dataset_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67dd29c",
   "metadata": {},
   "source": [
    "## 2. Data Distribution Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3149a7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_emotion_distribution(train_stats, test_stats):\n",
    "    \"\"\"Create comprehensive distribution plots\"\"\"\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Color mapping\n",
    "    colors = {'angry': '#ff4444', 'disgust': '#aa44aa', 'fear': '#444444', \n",
    "              'happy': '#44ff44', 'neutral': '#888888', 'sad': '#4444ff', 'surprise': '#ffaa44'}\n",
    "    \n",
    "    # 1. Training data distribution\n",
    "    emotions = list(train_stats.keys())\n",
    "    counts = list(train_stats.values())\n",
    "    bar_colors = [colors.get(emotion, '#666666') for emotion in emotions]\n",
    "    \n",
    "    bars1 = axes[0,0].bar(emotions, counts, color=bar_colors, alpha=0.8)\n",
    "    axes[0,0].set_title('Training Data Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[0,0].set_ylabel('Number of Images')\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, count in zip(bars1, counts):\n",
    "        axes[0,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 50,\n",
    "                      str(count), ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 2. Test data distribution\n",
    "    test_emotions = list(test_stats.keys())\n",
    "    test_counts = list(test_stats.values())\n",
    "    test_colors = [colors.get(emotion, '#666666') for emotion in test_emotions]\n",
    "    \n",
    "    bars2 = axes[0,1].bar(test_emotions, test_counts, color=test_colors, alpha=0.8)\n",
    "    axes[0,1].set_title('Test Data Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[0,1].set_ylabel('Number of Images')\n",
    "    axes[0,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, count in zip(bars2, test_counts):\n",
    "        axes[0,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 20,\n",
    "                      str(count), ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 3. Music-relevant vs excluded emotions\n",
    "    music_relevant_counts = sum([train_stats.get(e, 0) for e in CONFIG['music_relevant_emotions']])\n",
    "    excluded_counts = sum([train_stats.get(e, 0) for e in CONFIG['excluded_emotions']])\n",
    "    \n",
    "    pie_data = [music_relevant_counts, excluded_counts]\n",
    "    pie_labels = ['Music-Relevant\\nEmotions', 'Excluded\\nEmotions']\n",
    "    pie_colors = ['#44aa44', '#aa4444']\n",
    "    \n",
    "    axes[1,0].pie(pie_data, labels=pie_labels, colors=pie_colors, autopct='%1.1f%%',\n",
    "                 startangle=90, textprops={'fontsize': 12, 'fontweight': 'bold'})\n",
    "    axes[1,0].set_title('Dataset Composition for Music Recommendation', \n",
    "                       fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # 4. Class imbalance analysis\n",
    "    music_emotions = [e for e in emotions if e in CONFIG['music_relevant_emotions']]\n",
    "    music_counts = [train_stats[e] for e in music_emotions]\n",
    "    music_colors = [colors[e] for e in music_emotions]\n",
    "    \n",
    "    bars4 = axes[1,1].bar(music_emotions, music_counts, color=music_colors, alpha=0.8)\n",
    "    axes[1,1].set_title('Music-Relevant Emotions Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[1,1].set_ylabel('Number of Images')\n",
    "    axes[1,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add imbalance ratio line\n",
    "    max_count = max(music_counts)\n",
    "    min_count = min(music_counts)\n",
    "    imbalance_ratio = max_count / min_count\n",
    "    axes[1,1].axhline(y=np.mean(music_counts), color='red', linestyle='--', alpha=0.7)\n",
    "    axes[1,1].text(0.02, 0.98, f'Imbalance Ratio: {imbalance_ratio:.2f}', \n",
    "                  transform=axes[1,1].transAxes, fontsize=10, \n",
    "                  bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
    "                  verticalalignment='top')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CONFIG['output_dir'], 'emotion_distribution_analysis.png'), \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return music_relevant_counts, excluded_counts, imbalance_ratio\n",
    "\n",
    "music_count, excluded_count, imbalance_ratio = plot_emotion_distribution(train_distribution, test_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac28adf",
   "metadata": {},
   "source": [
    "## 3. Image Quality and Characteristics Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fcb1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_image_characteristics():\n",
    "    \"\"\"Analyze image quality, size distribution, and pixel statistics\"\"\"\n",
    "    \n",
    "    image_stats = {\n",
    "        'sizes': [],\n",
    "        'pixel_means': [],\n",
    "        'pixel_stds': [],\n",
    "        'brightness': [],\n",
    "        'contrast': [],\n",
    "        'emotion_stats': defaultdict(lambda: {'means': [], 'stds': [], 'brightness': []})\n",
    "    }\n",
    "    \n",
    "    print(\"Analyzing image characteristics...\")\n",
    "    \n",
    "    # Sample images from each emotion for analysis\n",
    "    sample_size = 100  # Sample size per emotion\n",
    "    \n",
    "    for emotion in CONFIG['music_relevant_emotions']:\n",
    "        emotion_path = os.path.join(CONFIG['train_dir'], emotion)\n",
    "        if os.path.exists(emotion_path):\n",
    "            image_files = [f for f in os.listdir(emotion_path) \n",
    "                          if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "            \n",
    "            # Sample random images\n",
    "            sample_files = np.random.choice(image_files, \n",
    "                                          min(sample_size, len(image_files)), \n",
    "                                          replace=False)\n",
    "            \n",
    "            for img_file in sample_files:\n",
    "                img_path = os.path.join(emotion_path, img_file)\n",
    "                try:\n",
    "                    # Load image\n",
    "                    image = Image.open(img_path)\n",
    "                    if image.mode != 'L':\n",
    "                        image = image.convert('L')\n",
    "                    \n",
    "                    # Convert to numpy array\n",
    "                    img_array = np.array(image)\n",
    "                    \n",
    "                    # Calculate statistics\n",
    "                    pixel_mean = np.mean(img_array)\n",
    "                    pixel_std = np.std(img_array)\n",
    "                    brightness = np.mean(img_array) / 255.0\n",
    "                    contrast = np.std(img_array) / 255.0\n",
    "                    \n",
    "                    # Store overall stats\n",
    "                    image_stats['sizes'].append(image.size)\n",
    "                    image_stats['pixel_means'].append(pixel_mean)\n",
    "                    image_stats['pixel_stds'].append(pixel_std)\n",
    "                    image_stats['brightness'].append(brightness)\n",
    "                    image_stats['contrast'].append(contrast)\n",
    "                    \n",
    "                    # Store emotion-specific stats\n",
    "                    image_stats['emotion_stats'][emotion]['means'].append(pixel_mean)\n",
    "                    image_stats['emotion_stats'][emotion]['stds'].append(pixel_std)\n",
    "                    image_stats['emotion_stats'][emotion]['brightness'].append(brightness)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {img_path}: {e}\")\n",
    "    \n",
    "    return image_stats\n",
    "\n",
    "image_characteristics = analyze_image_characteristics()\n",
    "print(f\"Analyzed {len(image_characteristics['pixel_means'])} images total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb850bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_characteristics(image_stats):\n",
    "    \"\"\"Plot image quality and characteristics analysis\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # 1. Pixel intensity distribution\n",
    "    axes[0,0].hist(image_stats['pixel_means'], bins=50, alpha=0.7, color='steelblue')\n",
    "    axes[0,0].set_title('Pixel Intensity Distribution', fontweight='bold')\n",
    "    axes[0,0].set_xlabel('Mean Pixel Intensity')\n",
    "    axes[0,0].set_ylabel('Frequency')\n",
    "    axes[0,0].axvline(np.mean(image_stats['pixel_means']), color='red', \n",
    "                     linestyle='--', label=f\"Mean: {np.mean(image_stats['pixel_means']):.1f}\")\n",
    "    axes[0,0].legend()\n",
    "    \n",
    "    # 2. Contrast distribution\n",
    "    axes[0,1].hist(image_stats['contrast'], bins=50, alpha=0.7, color='orange')\n",
    "    axes[0,1].set_title('Contrast Distribution', fontweight='bold')\n",
    "    axes[0,1].set_xlabel('Contrast (Normalized)')\n",
    "    axes[0,1].set_ylabel('Frequency')\n",
    "    axes[0,1].axvline(np.mean(image_stats['contrast']), color='red', \n",
    "                     linestyle='--', label=f\"Mean: {np.mean(image_stats['contrast']):.3f}\")\n",
    "    axes[0,1].legend()\n",
    "    \n",
    "    # 3. Brightness distribution\n",
    "    axes[0,2].hist(image_stats['brightness'], bins=50, alpha=0.7, color='gold')\n",
    "    axes[0,2].set_title('Brightness Distribution', fontweight='bold')\n",
    "    axes[0,2].set_xlabel('Brightness (Normalized)')\n",
    "    axes[0,2].set_ylabel('Frequency')\n",
    "    axes[0,2].axvline(np.mean(image_stats['brightness']), color='red', \n",
    "                     linestyle='--', label=f\"Mean: {np.mean(image_stats['brightness']):.3f}\")\n",
    "    axes[0,2].legend()\n",
    "    \n",
    "    # 4. Emotion-wise brightness comparison\n",
    "    emotions = list(image_stats['emotion_stats'].keys())\n",
    "    brightness_data = [image_stats['emotion_stats'][emotion]['brightness'] for emotion in emotions]\n",
    "    \n",
    "    bp1 = axes[1,0].boxplot(brightness_data, labels=emotions, patch_artist=True)\n",
    "    axes[1,0].set_title('Brightness Distribution by Emotion', fontweight='bold')\n",
    "    axes[1,0].set_ylabel('Brightness')\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Color the boxes\n",
    "    colors = ['#ff4444', '#44ff44', '#888888', '#4444ff', '#ffaa44']\n",
    "    for patch, color in zip(bp1['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "    \n",
    "    # 5. Emotion-wise contrast comparison\n",
    "    contrast_data = [np.array(image_stats['emotion_stats'][emotion]['stds'])/255.0 for emotion in emotions]\n",
    "    \n",
    "    bp2 = axes[1,1].boxplot(contrast_data, labels=emotions, patch_artist=True)\n",
    "    axes[1,1].set_title('Contrast Distribution by Emotion', fontweight='bold')\n",
    "    axes[1,1].set_ylabel('Contrast')\n",
    "    axes[1,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Color the boxes\n",
    "    for patch, color in zip(bp2['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "    \n",
    "    # 6. Statistical summary\n",
    "    axes[1,2].axis('off')\n",
    "    summary_text = f\"\"\"\n",
    "Image Characteristics Summary:\n",
    "\n",
    "Overall Statistics:\n",
    "• Mean Pixel Intensity: {np.mean(image_stats['pixel_means']):.2f} ± {np.std(image_stats['pixel_means']):.2f}\n",
    "• Mean Brightness: {np.mean(image_stats['brightness']):.3f} ± {np.std(image_stats['brightness']):.3f}\n",
    "• Mean Contrast: {np.mean(image_stats['contrast']):.3f} ± {np.std(image_stats['contrast']):.3f}\n",
    "\n",
    "Quality Indicators:\n",
    "• Low Contrast Images: {sum(1 for c in image_stats['contrast'] if c < 0.1)}\n",
    "• Very Dark Images: {sum(1 for b in image_stats['brightness'] if b < 0.2)}\n",
    "• Very Bright Images: {sum(1 for b in image_stats['brightness'] if b > 0.8)}\n",
    "\n",
    "Recommendations:\n",
    "• Apply histogram equalization for low contrast images\n",
    "• Use brightness augmentation during training\n",
    "• Consider gamma correction for extreme brightness\n",
    "    \"\"\"\n",
    "    \n",
    "    axes[1,2].text(0.05, 0.95, summary_text, transform=axes[1,2].transAxes,\n",
    "                  fontsize=10, verticalalignment='top',\n",
    "                  bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CONFIG['output_dir'], 'image_characteristics_analysis.png'), \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_image_characteristics(image_characteristics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695f2adb",
   "metadata": {},
   "source": [
    "## 4. Sample Images Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aedbc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sample_images():\n",
    "    \"\"\"Display sample images from each emotion class\"\"\"\n",
    "    \n",
    "    n_samples = 8  # Number of samples per emotion\n",
    "    emotions = CONFIG['music_relevant_emotions']\n",
    "    \n",
    "    fig, axes = plt.subplots(len(emotions), n_samples, figsize=(20, 15))\n",
    "    \n",
    "    for i, emotion in enumerate(emotions):\n",
    "        emotion_path = os.path.join(CONFIG['train_dir'], emotion)\n",
    "        if os.path.exists(emotion_path):\n",
    "            image_files = [f for f in os.listdir(emotion_path) \n",
    "                          if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "            \n",
    "            # Sample random images\n",
    "            sample_files = np.random.choice(image_files, \n",
    "                                          min(n_samples, len(image_files)), \n",
    "                                          replace=False)\n",
    "            \n",
    "            for j, img_file in enumerate(sample_files):\n",
    "                img_path = os.path.join(emotion_path, img_file)\n",
    "                try:\n",
    "                    image = Image.open(img_path)\n",
    "                    if image.mode != 'L':\n",
    "                        image = image.convert('L')\n",
    "                    \n",
    "                    axes[i,j].imshow(image, cmap='gray')\n",
    "                    axes[i,j].axis('off')\n",
    "                    \n",
    "                    if j == 0:\n",
    "                        axes[i,j].set_ylabel(emotion.upper(), fontsize=14, fontweight='bold')\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    axes[i,j].text(0.5, 0.5, 'Error', ha='center', va='center')\n",
    "                    axes[i,j].axis('off')\n",
    "    \n",
    "    plt.suptitle('Sample Images from Each Emotion Class (Music-Relevant)', \n",
    "                fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CONFIG['output_dir'], 'sample_images_by_emotion.png'), \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "visualize_sample_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356c53d3",
   "metadata": {},
   "source": [
    "## 5. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528e0128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_data_quality():\n",
    "    \"\"\"Assess data quality issues and provide recommendations\"\"\"\n",
    "    \n",
    "    quality_report = {\n",
    "        'corrupted_images': [],\n",
    "        'size_inconsistencies': [],\n",
    "        'very_dark_images': [],\n",
    "        'very_bright_images': [],\n",
    "        'low_contrast_images': []\n",
    "    }\n",
    "    \n",
    "    print(\"Assessing data quality...\")\n",
    "    \n",
    "    total_images = 0\n",
    "    processed_images = 0\n",
    "    \n",
    "    for emotion in CONFIG['music_relevant_emotions']:\n",
    "        emotion_path = os.path.join(CONFIG['train_dir'], emotion)\n",
    "        if os.path.exists(emotion_path):\n",
    "            image_files = [f for f in os.listdir(emotion_path) \n",
    "                          if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "            \n",
    "            for img_file in image_files[:200]:  # Sample first 200 images per emotion\n",
    "                total_images += 1\n",
    "                img_path = os.path.join(emotion_path, img_file)\n",
    "                \n",
    "                try:\n",
    "                    image = Image.open(img_path)\n",
    "                    \n",
    "                    # Check size consistency\n",
    "                    if image.size != (48, 48):\n",
    "                        quality_report['size_inconsistencies'].append({\n",
    "                            'path': img_path,\n",
    "                            'size': image.size,\n",
    "                            'emotion': emotion\n",
    "                        })\n",
    "                    \n",
    "                    # Convert to grayscale and analyze\n",
    "                    if image.mode != 'L':\n",
    "                        image = image.convert('L')\n",
    "                    \n",
    "                    img_array = np.array(image)\n",
    "                    brightness = np.mean(img_array) / 255.0\n",
    "                    contrast = np.std(img_array) / 255.0\n",
    "                    \n",
    "                    # Check for quality issues\n",
    "                    if brightness < 0.15:\n",
    "                        quality_report['very_dark_images'].append({\n",
    "                            'path': img_path,\n",
    "                            'brightness': brightness,\n",
    "                            'emotion': emotion\n",
    "                        })\n",
    "                    \n",
    "                    if brightness > 0.85:\n",
    "                        quality_report['very_bright_images'].append({\n",
    "                            'path': img_path,\n",
    "                            'brightness': brightness,\n",
    "                            'emotion': emotion\n",
    "                        })\n",
    "                    \n",
    "                    if contrast < 0.08:\n",
    "                        quality_report['low_contrast_images'].append({\n",
    "                            'path': img_path,\n",
    "                            'contrast': contrast,\n",
    "                            'emotion': emotion\n",
    "                        })\n",
    "                    \n",
    "                    processed_images += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    quality_report['corrupted_images'].append({\n",
    "                        'path': img_path,\n",
    "                        'error': str(e),\n",
    "                        'emotion': emotion\n",
    "                    })\n",
    "    \n",
    "    # Generate quality report\n",
    "    print(\"\\nData Quality Assessment Report\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Total Images Analyzed: {total_images}\")\n",
    "    print(f\"Successfully Processed: {processed_images}\")\n",
    "    print(f\"Corrupted Images: {len(quality_report['corrupted_images'])}\")\n",
    "    print(f\"Size Inconsistencies: {len(quality_report['size_inconsistencies'])}\")\n",
    "    print(f\"Very Dark Images: {len(quality_report['very_dark_images'])}\")\n",
    "    print(f\"Very Bright Images: {len(quality_report['very_bright_images'])}\")\n",
    "    print(f\"Low Contrast Images: {len(quality_report['low_contrast_images'])}\")\n",
    "    \n",
    "    return quality_report\n",
    "\n",
    "quality_assessment = assess_data_quality()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de90493",
   "metadata": {},
   "source": [
    "## 6. Recommendations for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb6b194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recommendations():\n",
    "    \"\"\"Generate comprehensive recommendations for model training\"\"\"\n",
    "    \n",
    "    recommendations = {\n",
    "        'data_preprocessing': [\n",
    "            \"Apply histogram equalization to improve contrast in low-contrast images\",\n",
    "            \"Use CLAHE (Contrast Limited Adaptive Histogram Equalization) for better local contrast\",\n",
    "            \"Implement gamma correction for extreme brightness variations\",\n",
    "            \"Apply noise reduction filters to improve image quality\"\n",
    "        ],\n",
    "        'data_augmentation': [\n",
    "            \"Use rotation (±15 degrees) to increase data diversity\",\n",
    "            \"Apply random horizontal flips (facial expressions are symmetric)\",\n",
    "            \"Implement brightness and contrast adjustments (±20%)\",\n",
    "            \"Add slight translation and affine transformations\",\n",
    "            \"Consider elastic deformations for better generalization\"\n",
    "        ],\n",
    "        'class_balancing': [\n",
    "            f\"Use weighted loss function (imbalance ratio: {imbalance_ratio:.2f})\",\n",
    "            \"Implement oversampling for minority classes (SMOTE or similar)\",\n",
    "            \"Consider focal loss to handle difficult examples\",\n",
    "            \"Use stratified sampling for train/validation splits\"\n",
    "        ],\n",
    "        'model_architecture': [\n",
    "            \"Use ResNet-18 with modified first layer for grayscale input\",\n",
    "            \"Add dropout layers (0.3-0.5) to prevent overfitting\",\n",
    "            \"Implement batch normalization for stable training\",\n",
    "            \"Use attention mechanisms to focus on facial features\",\n",
    "            \"Consider ensemble methods for better accuracy\"\n",
    "        ],\n",
    "        'training_strategy': [\n",
    "            \"Use transfer learning with ImageNet pretrained weights\",\n",
    "            \"Implement learning rate scheduling (reduce on plateau)\",\n",
    "            \"Use early stopping to prevent overfitting\",\n",
    "            \"Apply gradient clipping for stable training\",\n",
    "            \"Monitor both accuracy and per-class F1 scores\"\n",
    "        ],\n",
    "        'music_recommendation': [\n",
    "            \"Map emotions to music genres: Happy→Pop/Dance, Sad→Blues/Ballads, Angry→Rock/Metal\",\n",
    "            \"Consider emotion intensity levels for nuanced recommendations\",\n",
    "            \"Implement temporal emotion tracking for dynamic playlists\",\n",
    "            \"Use confidence scores to handle uncertain predictions\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(\"\\nCOMPREHENSIVE RECOMMENDATIONS FOR EMOTION DETECTION MODEL\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for category, items in recommendations.items():\n",
    "        print(f\"\\n{category.replace('_', ' ').title()}:\")\n",
    "        for i, item in enumerate(items, 1):\n",
    "            print(f\"  {i}. {item}\")\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "final_recommendations = generate_recommendations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a1137e",
   "metadata": {},
   "source": [
    "## 7. Save Analysis Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10eb697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_analysis_results():\n",
    "    \"\"\"Save all analysis results to JSON file\"\"\"\n",
    "    \n",
    "    analysis_results = {\n",
    "        'dataset_info': {\n",
    "            'original_emotions': CONFIG['original_emotions'],\n",
    "            'music_relevant_emotions': CONFIG['music_relevant_emotions'],\n",
    "            'excluded_emotions': CONFIG['excluded_emotions'],\n",
    "            'train_distribution': train_distribution,\n",
    "            'test_distribution': test_distribution,\n",
    "            'total_music_relevant': music_count,\n",
    "            'total_excluded': excluded_count,\n",
    "            'class_imbalance_ratio': float(imbalance_ratio)\n",
    "        },\n",
    "        'image_characteristics': {\n",
    "            'mean_pixel_intensity': float(np.mean(image_characteristics['pixel_means'])),\n",
    "            'std_pixel_intensity': float(np.std(image_characteristics['pixel_means'])),\n",
    "            'mean_brightness': float(np.mean(image_characteristics['brightness'])),\n",
    "            'std_brightness': float(np.std(image_characteristics['brightness'])),\n",
    "            'mean_contrast': float(np.mean(image_characteristics['contrast'])),\n",
    "            'std_contrast': float(np.std(image_characteristics['contrast']))\n",
    "        },\n",
    "        'quality_assessment': {\n",
    "            'corrupted_images_count': len(quality_assessment['corrupted_images']),\n",
    "            'size_inconsistencies_count': len(quality_assessment['size_inconsistencies']),\n",
    "            'very_dark_images_count': len(quality_assessment['very_dark_images']),\n",
    "            'very_bright_images_count': len(quality_assessment['very_bright_images']),\n",
    "            'low_contrast_images_count': len(quality_assessment['low_contrast_images'])\n",
    "        },\n",
    "        'recommendations': final_recommendations,\n",
    "        'analysis_date': pd.Timestamp.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    # Save to JSON\n",
    "    output_file = os.path.join(CONFIG['output_dir'], 'eda_analysis_results.json')\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(analysis_results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nAnalysis results saved to: {output_file}\")\n",
    "    \n",
    "    # Create summary report\n",
    "    summary_report = f\"\"\"\n",
    "EMOTION DETECTION DATASET ANALYSIS SUMMARY\n",
    "==========================================\n",
    "\n",
    "Dataset Overview:\n",
    "- Total emotions: {len(CONFIG['original_emotions'])}\n",
    "- Music-relevant emotions: {len(CONFIG['music_relevant_emotions'])}\n",
    "- Excluded emotions: {len(CONFIG['excluded_emotions'])}\n",
    "- Class imbalance ratio: {imbalance_ratio:.2f}\n",
    "\n",
    "Data Quality:\n",
    "- Mean brightness: {np.mean(image_characteristics['brightness']):.3f}\n",
    "- Mean contrast: {np.mean(image_characteristics['contrast']):.3f}\n",
    "- Quality issues identified: {len(quality_assessment['corrupted_images']) + len(quality_assessment['low_contrast_images'])}\n",
    "\n",
    "Key Recommendations:\n",
    "1. Focus on {', '.join(CONFIG['music_relevant_emotions'])} emotions for music recommendation\n",
    "2. Apply class balancing techniques due to imbalance ratio of {imbalance_ratio:.2f}\n",
    "3. Use histogram equalization and contrast enhancement\n",
    "4. Implement comprehensive data augmentation strategy\n",
    "5. Use ResNet-18 with attention mechanisms for better feature learning\n",
    "\n",
    "Output Directory: {CONFIG['output_dir']}\n",
    "Analysis Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "    \"\"\"\n",
    "    \n",
    "    summary_file = os.path.join(CONFIG['output_dir'], 'eda_summary_report.txt')\n",
    "    with open(summary_file, 'w') as f:\n",
    "        f.write(summary_report)\n",
    "    \n",
    "    print(f\"Summary report saved to: {summary_file}\")\n",
    "    print(\"\\nEDA Analysis Complete!\")\n",
    "\n",
    "save_analysis_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44aefc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7538e468",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emotion-detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
