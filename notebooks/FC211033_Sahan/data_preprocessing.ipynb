{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d81f901d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotion Detection Preprocessing Notebook\n",
    "# ResNet-18 Architecture for 48x48 Grayscale Images\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.utils import make_grid\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d9d286f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Setup Complete!\n",
      "PyTorch Version: 2.1.0+cu121\n",
      "CUDA Available: False\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "print(\"Environment Setup Complete!\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb15740b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIRECTORY CONFIGURATION \n",
    "\n",
    "CONFIG = {\n",
    "    # Main dataset directory\n",
    "    'dataset_root': '../../data/raw/fer2013/train',\n",
    "    \n",
    "    'train_dir': '../../data/raw/fer2013/train',\n",
    "    'test_dir': '../../data/raw/fer2013/test',\n",
    "\n",
    "# Output directories\n",
    "    'output_dir': '../../data/processed_data',\n",
    "    'model_save_dir': '../../models',\n",
    "    \n",
    "    # Dataset parameters (since images are already 48x48 grayscale)\n",
    "    'image_size': (48, 48),\n",
    "    'batch_size': 32,\n",
    "    'validation_split': 0.2,\n",
    "    'test_split': 0.1,\n",
    "    \n",
    "    # Training parameters\n",
    "    'num_epochs': 50,\n",
    "    'learning_rate': 0.001,\n",
    "    'weight_decay': 1e-4,\n",
    "    'patience': 10,\n",
    "    \n",
    "    # Emotion labels (modify according to your dataset)\n",
    "    'emotion_labels': ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2ce3024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create output directories\n",
    "for dir_path in [CONFIG['output_dir'], CONFIG['model_save_dir']]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "print(\"Configuration loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc023125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EFFICIENT DATASET CLASS (No EDA)\n",
    "# =============================================================================\n",
    "\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        \n",
    "        # Load images from directory structure\n",
    "        for emotion in CONFIG['emotion_labels']:\n",
    "            emotion_dir = os.path.join(data_dir, emotion)\n",
    "            if os.path.exists(emotion_dir):\n",
    "                for img_file in os.listdir(emotion_dir):\n",
    "                    if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                        img_path = os.path.join(emotion_dir, img_file)\n",
    "                        self.data.append(img_path)\n",
    "                        self.labels.append(emotion)\n",
    "        \n",
    "        # Encode labels\n",
    "        self.labels = self.label_encoder.fit_transform(self.labels)\n",
    "        print(f\"Loaded {len(self.data)} images across {len(self.label_encoder.classes_)} classes\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Load image (already grayscale 48x48)\n",
    "        image = Image.open(img_path)\n",
    "        \n",
    "        # Safety checks (minimal since images are pre-processed)\n",
    "        if image.mode != 'L':\n",
    "            image = image.convert('L')\n",
    "        if image.size != CONFIG['image_size']:\n",
    "            image = image.resize(CONFIG['image_size'])\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "983cd182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# OPTIMIZED TRANSFORMS (No Resize Needed)\n",
    "# =============================================================================\n",
    "\n",
    "def get_transforms():\n",
    "    \"\"\"Optimized transforms for pre-processed 48x48 grayscale images\"\"\"\n",
    "    \n",
    "    # Training transforms with augmentation\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485], std=[0.229])\n",
    "    ])\n",
    "    \n",
    "    # Validation/Test transforms (minimal processing)\n",
    "    val_transforms = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485], std=[0.229])\n",
    "    ])\n",
    "    \n",
    "    return train_transforms, val_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "180a6854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RESNET-18 MODEL FOR GRAYSCALE EMOTION DETECTION\n",
    "# =============================================================================\n",
    "\n",
    "class EmotionResNet(nn.Module):\n",
    "    def __init__(self, num_classes=7, pretrained=True):\n",
    "        super(EmotionResNet, self).__init__()\n",
    "        \n",
    "        # Load pretrained ResNet-18\n",
    "        self.resnet = models.resnet18(pretrained=pretrained)\n",
    "        \n",
    "        # Modify first layer for grayscale input\n",
    "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        \n",
    "        # Modify final layer for emotion classification\n",
    "        num_features = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize the new conv1 layer\n",
    "        nn.init.kaiming_normal_(self.resnet.conv1.weight, mode='fan_out', nonlinearity='relu')\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "021bdd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAINING SETUP FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def setup_training(model, device):\n",
    "    \"\"\"Setup training components\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=CONFIG['learning_rate'],\n",
    "        weight_decay=CONFIG['weight_decay']\n",
    "    )\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=CONFIG['patience']//2, verbose=True\n",
    "    )\n",
    "    return criterion, optimizer, scheduler\n",
    "\n",
    "def calculate_dataset_statistics(data_loader):\n",
    "    \"\"\"Calculate actual mean and std of your dataset\"\"\"\n",
    "    print(\"Calculating dataset statistics...\")\n",
    "    \n",
    "    mean = torch.zeros(1)\n",
    "    std = torch.zeros(1)\n",
    "    total_samples = 0\n",
    "    \n",
    "    for images, _ in data_loader:\n",
    "        batch_samples = images.size(0)\n",
    "        images = images.view(batch_samples, images.size(1), -1)\n",
    "        mean += images.mean(2).sum(0)\n",
    "        std += images.std(2).sum(0)\n",
    "        total_samples += batch_samples\n",
    "    \n",
    "    mean /= total_samples\n",
    "    std /= total_samples\n",
    "    \n",
    "    print(f\"Your dataset - Mean: {mean.item():.4f}, Std: {std.item():.4f}\")\n",
    "    return mean.item(), std.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3a7eef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MAIN PREPROCESSING PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "def create_data_loaders():\n",
    "    \"\"\"Create train/validation/test data loaders\"\"\"\n",
    "    \n",
    "    print(\"Creating data loaders...\")\n",
    "    \n",
    "    # Load dataset\n",
    "    dataset = EmotionDataset(CONFIG['dataset_root'])\n",
    "    \n",
    "    # Create data splits\n",
    "    train_idx, temp_idx = train_test_split(\n",
    "        range(len(dataset)), \n",
    "        test_size=CONFIG['validation_split'] + CONFIG['test_split'],\n",
    "        stratify=dataset.labels, random_state=42\n",
    "    )\n",
    "    \n",
    "    val_idx, test_idx = train_test_split(\n",
    "        temp_idx,\n",
    "        test_size=CONFIG['test_split'] / (CONFIG['validation_split'] + CONFIG['test_split']),\n",
    "        stratify=[dataset.labels[i] for i in temp_idx], random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Data splits - Train: {len(train_idx)}, Val: {len(val_idx)}, Test: {len(test_idx)}\")\n",
    "    \n",
    "    # Get transforms\n",
    "    train_transforms, val_transforms = get_transforms()\n",
    "    \n",
    "    # Create subset datasets\n",
    "    train_dataset = torch.utils.data.Subset(dataset, train_idx)\n",
    "    val_dataset = torch.utils.data.Subset(dataset, val_idx)\n",
    "    test_dataset = torch.utils.data.Subset(dataset, test_idx)\n",
    "    \n",
    "    # Apply transforms\n",
    "    train_dataset.dataset.transform = train_transforms\n",
    "    val_dataset.dataset.transform = val_transforms\n",
    "    test_dataset.dataset.transform = val_transforms\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], \n",
    "                            shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], \n",
    "                          shuffle=False, num_workers=4, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], \n",
    "                           shuffle=False, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, dataset\n",
    "\n",
    "def initialize_model():\n",
    "    \"\"\"Initialize the ResNet-18 model\"\"\"\n",
    "    \n",
    "    print(\"Initializing model...\")\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    model = EmotionResNet(num_classes=len(CONFIG['emotion_labels']))\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Model summary\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    return model, device\n",
    "\n",
    "def save_preprocessing_info(dataset, train_size, val_size, test_size):\n",
    "    \"\"\"Save preprocessing information\"\"\"\n",
    "    \n",
    "    import json\n",
    "    \n",
    "    preprocessing_info = {\n",
    "        'dataset_size': len(dataset),\n",
    "        'train_size': train_size,\n",
    "        'val_size': val_size,\n",
    "        'test_size': test_size,\n",
    "        'num_classes': len(CONFIG['emotion_labels']),\n",
    "        'emotion_labels': CONFIG['emotion_labels'],\n",
    "        'image_size': CONFIG['image_size'],\n",
    "        'batch_size': CONFIG['batch_size'],\n",
    "        'label_encoder_classes': dataset.label_encoder.classes_.tolist(),\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(CONFIG['output_dir'], 'preprocessing_info.json'), 'w') as f:\n",
    "        json.dump(preprocessing_info, f, indent=2)\n",
    "    \n",
    "    print(f\"Preprocessing info saved to: {os.path.join(CONFIG['output_dir'], 'preprocessing_info.json')}\")\n",
    "    return preprocessing_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35f09ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MAIN EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main preprocessing pipeline execution\"\"\"\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    print(\"EMOTION DETECTION PREPROCESSING\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader, val_loader, test_loader, dataset = create_data_loaders()\n",
    "    \n",
    "    # Initialize model\n",
    "    model, device = initialize_model()\n",
    "    \n",
    "    # Setup training components\n",
    "    criterion, optimizer, scheduler = setup_training(model, device)\n",
    "    \n",
    "    # Calculate dataset statistics (optional - for fine-tuning normalization)\n",
    "    print(\"\\nCalculating dataset statistics...\")\n",
    "    temp_loader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "    mean, std = calculate_dataset_statistics(temp_loader)\n",
    "    print(f\"Consider updating normalization to: mean=[{mean:.4f}], std=[{std:.4f}]\")\n",
    "    \n",
    "    # Save preprocessing information\n",
    "    save_preprocessing_info(dataset, len(train_loader.dataset), \n",
    "                          len(val_loader.dataset), len(test_loader.dataset))\n",
    "    \n",
    "    print(\"\\nPreprocessing completed successfully!\")\n",
    "    print(\"Ready for training!\")\n",
    "    \n",
    "    return {\n",
    "        'train_loader': train_loader,\n",
    "        'val_loader': val_loader,\n",
    "        'test_loader': test_loader,\n",
    "        'model': model,\n",
    "        'device': device,\n",
    "        'criterion': criterion,\n",
    "        'optimizer': optimizer,\n",
    "        'scheduler': scheduler,\n",
    "        'dataset_stats': {'mean': mean, 'std': std}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072c2106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "EMOTION DETECTION PREPROCESSING\n",
      "==================================================\n",
      "Creating data loaders...\n",
      "Loaded 28709 images across 7 classes\n",
      "Data splits - Train: 20096, Val: 5742, Test: 2871\n",
      "Initializing model...\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 11,436,487\n",
      "Trainable parameters: 11,436,487\n",
      "\n",
      "Calculating dataset statistics...\n",
      "Calculating dataset statistics...\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# USAGE EXAMPLE\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    \n",
    "    results = main()\n",
    "    \n",
    "    # Example usage after running main():\n",
    "    \"\"\"\n",
    "    # Get the preprocessed components\n",
    "    train_loader = results['train_loader']\n",
    "    val_loader = results['val_loader']\n",
    "    test_loader = results['test_loader']\n",
    "    model = results['model']\n",
    "    device = results['device']\n",
    "    criterion = results['criterion']\n",
    "    optimizer = results['optimizer']\n",
    "    scheduler = results['scheduler']\n",
    "    \n",
    "    # Ready for training loop!\n",
    "    # Your training code goes here...\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2a0b2a-db76-4f13-bd18-098ef658c4b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
