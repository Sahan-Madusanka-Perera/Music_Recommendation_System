{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f72df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Emotion Detection Preprocessing for Music Recommendation System\n",
    "# Optimized ResNet-18 Architecture for 48x48 Grayscale Facial Emotion Recognition\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image, ImageEnhance\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.utils import make_grid\n",
    "from collections import Counter, defaultdict\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a1375a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENHANCED CONFIGURATION FOR MUSIC RECOMMENDATION SYSTEM\n",
    "\n",
    "CONFIG = {\n",
    "    # Directory paths - Updated for correct output location\n",
    "    'dataset_root': '../../data/raw/fer2013/train',\n",
    "    'train_dir': '../../data/raw/fer2013/train',\n",
    "    'test_dir': '../../data/raw/fer2013/test',\n",
    "    'output_dir': '../../data/processed/FC211033_Sahan',\n",
    "    'model_save_dir': '../../models',\n",
    "    \n",
    "    # Dataset parameters\n",
    "    'image_size': (48, 48),\n",
    "    'batch_size': 64,  # Increased for better gradient estimates\n",
    "    'validation_split': 0.2,\n",
    "    'test_split': 0.1,\n",
    "    \n",
    "    # Training parameters - Optimized for ResNet-18\n",
    "    'num_epochs': 100,\n",
    "    'learning_rate': 0.001,\n",
    "    'weight_decay': 1e-4,\n",
    "    'patience': 15,  # Increased patience\n",
    "    'lr_scheduler_patience': 8,\n",
    "    'lr_scheduler_factor': 0.5,\n",
    "    \n",
    "    # Original emotion labels from FER2013\n",
    "    'original_emotions': ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise'],\n",
    "    \n",
    "    # Music-relevant emotions (excluding fear and disgust for better music mapping)\n",
    "    'music_emotions': ['angry', 'happy', 'neutral', 'sad', 'surprise'],\n",
    "    \n",
    "    # Music mapping for recommendations\n",
    "    'emotion_to_music': {\n",
    "        'happy': ['Pop', 'Dance', 'Upbeat', 'Electronic'],\n",
    "        'sad': ['Blues', 'Ballads', 'Acoustic', 'Melancholic'],\n",
    "        'angry': ['Rock', 'Metal', 'Punk', 'Aggressive'],\n",
    "        'neutral': ['Classical', 'Ambient', 'Instrumental', 'Chill'],\n",
    "        'surprise': ['Experimental', 'Fusion', 'Eclectic', 'Dynamic']\n",
    "    },\n",
    "    \n",
    "    # Data augmentation parameters\n",
    "    'augmentation': {\n",
    "        'rotation_degrees': 15,\n",
    "        'horizontal_flip_prob': 0.5,\n",
    "        'brightness_factor': 0.2,\n",
    "        'contrast_factor': 0.2,\n",
    "        'translate': (0.1, 0.1),\n",
    "        'scale': (0.9, 1.1)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create output directories\n",
    "for dir_path in [CONFIG['output_dir'], CONFIG['model_save_dir']]:\n",
    "    os.makedirs(dir_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659fc25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENHANCED DATASET CLASS WITH DATA PROCESSING AND COPYING\n",
    "\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "class MusicEmotionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Enhanced dataset class for music recommendation emotion detection.\n",
    "    Processes and copies quality-filtered images to processed directory.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, transform=None, apply_clahe=True, filter_quality=True, \n",
    "                 copy_to_processed=True, processed_dir=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.apply_clahe = apply_clahe\n",
    "        self.filter_quality = filter_quality\n",
    "        self.copy_to_processed = copy_to_processed\n",
    "        self.processed_dir = processed_dir or CONFIG['output_dir']\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        \n",
    "        # Initialize CLAHE for contrast enhancement\n",
    "        self.clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "        \n",
    "        # Load and process music-relevant emotions\n",
    "        self._process_emotions()\n",
    "        \n",
    "        # Encode labels\n",
    "        self.labels = self.label_encoder.fit_transform(self.labels)\n",
    "    \n",
    "    def _process_emotions(self):\n",
    "        \"\"\"Process and copy quality-filtered images\"\"\"\n",
    "        total_loaded = 0\n",
    "        total_filtered = 0\n",
    "        \n",
    "        print(\"Processing music-relevant emotions...\")\n",
    "        for emotion in CONFIG['music_emotions']:\n",
    "            emotion_dir = os.path.join(self.data_dir, emotion)\n",
    "            if os.path.exists(emotion_dir):\n",
    "                loaded, filtered = self._process_emotion_images(emotion_dir, emotion)\n",
    "                total_loaded += loaded\n",
    "                total_filtered += filtered\n",
    "        \n",
    "        print(f\"Processed {total_loaded} quality images, filtered out {total_filtered} low-quality images\")\n",
    "        \n",
    "    def _process_emotion_images(self, emotion_dir, emotion):\n",
    "        \"\"\"Process and optionally copy images for an emotion\"\"\"\n",
    "        loaded_count = 0\n",
    "        filtered_count = 0\n",
    "        \n",
    "        # Create processed emotion directory\n",
    "        if self.copy_to_processed:\n",
    "            processed_emotion_dir = os.path.join(self.processed_dir, emotion)\n",
    "            os.makedirs(processed_emotion_dir, exist_ok=True)\n",
    "        \n",
    "        image_files = [f for f in os.listdir(emotion_dir) \n",
    "                      if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        \n",
    "        for img_file in tqdm(image_files, desc=f\"Processing {emotion}\", leave=False):\n",
    "            img_path = os.path.join(emotion_dir, img_file)\n",
    "            \n",
    "            if self.filter_quality and not self._is_quality_image(img_path):\n",
    "                filtered_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Process and copy image\n",
    "            if self.copy_to_processed:\n",
    "                processed_path = os.path.join(processed_emotion_dir, img_file)\n",
    "                self._process_and_save_image(img_path, processed_path)\n",
    "                self.data.append(processed_path)\n",
    "            else:\n",
    "                self.data.append(img_path)\n",
    "            \n",
    "            self.labels.append(emotion)\n",
    "            loaded_count += 1\n",
    "        \n",
    "        return loaded_count, filtered_count\n",
    "    \n",
    "    def _process_and_save_image(self, src_path, dst_path):\n",
    "        \"\"\"Process image with quality enhancements and save\"\"\"\n",
    "        try:\n",
    "            # Load image\n",
    "            image = Image.open(src_path)\n",
    "            if image.mode != 'L':\n",
    "                image = image.convert('L')\n",
    "            \n",
    "            # Resize if needed\n",
    "            if image.size != CONFIG['image_size']:\n",
    "                image = image.resize(CONFIG['image_size'], Image.LANCZOS)\n",
    "            \n",
    "            # Apply CLAHE for contrast enhancement\n",
    "            if self.apply_clahe:\n",
    "                img_array = np.array(image)\n",
    "                img_array = self.clahe.apply(img_array)\n",
    "                image = Image.fromarray(img_array)\n",
    "            \n",
    "            # Save processed image\n",
    "            image.save(dst_path, 'PNG', optimize=True)\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Fallback: copy original if processing fails\n",
    "            shutil.copy2(src_path, dst_path)\n",
    "    \n",
    "    def _is_quality_image(self, img_path):\n",
    "        \"\"\"Filter out low quality images\"\"\"\n",
    "        try:\n",
    "            image = Image.open(img_path)\n",
    "            if image.mode != 'L':\n",
    "                image = image.convert('L')\n",
    "            \n",
    "            img_array = np.array(image)\n",
    "            \n",
    "            # Quality criteria\n",
    "            brightness = np.mean(img_array) / 255.0\n",
    "            contrast = np.std(img_array) / 255.0\n",
    "            \n",
    "            # Filter criteria for music emotion detection\n",
    "            if brightness < 0.1 or brightness > 0.95:  # Too dark or too bright\n",
    "                return False\n",
    "            if contrast < 0.05:  # Too low contrast\n",
    "                return False\n",
    "            \n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def get_class_weights(self):\n",
    "        \"\"\"Calculate balanced class weights\"\"\"\n",
    "        class_weights = compute_class_weight(\n",
    "            'balanced',\n",
    "            classes=np.unique(self.labels),\n",
    "            y=self.labels\n",
    "        )\n",
    "        return torch.FloatTensor(class_weights)\n",
    "    \n",
    "    def get_weighted_sampler(self):\n",
    "        \"\"\"Create weighted sampler for balanced training\"\"\"\n",
    "        class_counts = Counter(self.labels)\n",
    "        weights = [1.0 / class_counts[label] for label in self.labels]\n",
    "        return WeightedRandomSampler(weights, len(weights))\n",
    "    \n",
    "    def get_statistics(self):\n",
    "        \"\"\"Get dataset statistics\"\"\"\n",
    "        class_counts = Counter(self.labels)\n",
    "        stats = {\n",
    "            'total_samples': len(self.labels),\n",
    "            'num_classes': len(self.label_encoder.classes_),\n",
    "            'class_distribution': {\n",
    "                self.label_encoder.classes_[i]: class_counts[i] \n",
    "                for i in range(len(self.label_encoder.classes_))\n",
    "            },\n",
    "            'emotions': list(self.label_encoder.classes_)\n",
    "        }\n",
    "        return stats\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(img_path)\n",
    "        if image.mode != 'L':\n",
    "            image = image.convert('L')\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0c97d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENHANCED TRANSFORMS WITH ADVANCED AUGMENTATION STRATEGIES\n",
    "\n",
    "class AdvancedTransforms:\n",
    "    \"\"\"Advanced data augmentation strategies optimized for facial emotion recognition\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_dataset_statistics(dataset):\n",
    "        \"\"\"Calculate dataset statistics for normalization\"\"\"\n",
    "        \n",
    "        pixel_values = []\n",
    "        sample_size = min(1000, len(dataset))  # Sample for efficiency\n",
    "        indices = np.random.choice(len(dataset), sample_size, replace=False)\n",
    "        \n",
    "        for idx in indices:\n",
    "            img_path = dataset.data[idx]\n",
    "            image = Image.open(img_path)\n",
    "            if image.mode != 'L':\n",
    "                image = image.convert('L')\n",
    "            if image.size != CONFIG['image_size']:\n",
    "                image = image.resize(CONFIG['image_size'], Image.LANCZOS)\n",
    "            \n",
    "            img_array = np.array(image, dtype=np.float32) / 255.0\n",
    "            pixel_values.extend(img_array.flatten())\n",
    "        \n",
    "        mean = np.mean(pixel_values)\n",
    "        std = np.std(pixel_values)\n",
    "        \n",
    "        return mean, std\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_train_transforms(mean=None, std=None):\n",
    "        \"\"\"Enhanced training transforms with comprehensive augmentation\"\"\"\n",
    "        if mean is None or std is None:\n",
    "            mean, std = 0.5, 0.25  # Default values for grayscale\n",
    "        \n",
    "        return transforms.Compose([\n",
    "            # Geometric augmentations\n",
    "            transforms.RandomRotation(\n",
    "                degrees=CONFIG['augmentation']['rotation_degrees'], \n",
    "                fill=128  # Gray fill for rotations\n",
    "            ),\n",
    "            transforms.RandomHorizontalFlip(p=CONFIG['augmentation']['horizontal_flip_prob']),\n",
    "            transforms.RandomAffine(\n",
    "                degrees=0,\n",
    "                translate=CONFIG['augmentation']['translate'],\n",
    "                scale=CONFIG['augmentation']['scale'],\n",
    "                fill=128\n",
    "            ),\n",
    "            \n",
    "            # Photometric augmentations\n",
    "            transforms.ColorJitter(\n",
    "                brightness=CONFIG['augmentation']['brightness_factor'],\n",
    "                contrast=CONFIG['augmentation']['contrast_factor']\n",
    "            ),\n",
    "            \n",
    "            # Random erasing for robustness\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[mean], std=[std]),\n",
    "            transforms.RandomErasing(p=0.1, scale=(0.02, 0.1), ratio=(0.3, 3.3)),\n",
    "        ])\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_val_transforms(mean=None, std=None):\n",
    "        \"\"\"Validation/test transforms - only normalization\"\"\"\n",
    "        if mean is None or std is None:\n",
    "            mean, std = 0.5, 0.25\n",
    "        \n",
    "        return transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[mean], std=[std])\n",
    "        ])\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_test_time_augmentation_transforms(mean=None, std=None):\n",
    "        \"\"\"Test-time augmentation for improved inference\"\"\"\n",
    "        if mean is None or std is None:\n",
    "            mean, std = 0.5, 0.25\n",
    "        \n",
    "        return [\n",
    "            # Original\n",
    "            transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[mean], std=[std])\n",
    "            ]),\n",
    "            # Slight rotation\n",
    "            transforms.Compose([\n",
    "                transforms.RandomRotation(degrees=5, fill=128),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[mean], std=[std])\n",
    "            ]),\n",
    "            # Horizontal flip\n",
    "            transforms.Compose([\n",
    "                transforms.RandomHorizontalFlip(p=1.0),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[mean], std=[std])\n",
    "            ])\n",
    "        ]\n",
    "\n",
    "class TransformedDataset(Dataset):\n",
    "    \"\"\"Wrapper for applying transforms to dataset subsets\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, indices, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.indices = indices\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        actual_idx = self.indices[idx]\n",
    "        img_path = self.dataset.data[actual_idx]\n",
    "        label = self.dataset.labels[actual_idx]\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(img_path)\n",
    "        if image.mode != 'L':\n",
    "            image = image.convert('L')\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ece643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dataset_statistics(dataset):\n",
    "    \"\"\"Calculate mean and std of the dataset\"\"\"\n",
    "    return AdvancedTransforms.calculate_dataset_statistics(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222c74f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(mean=None, std=None):\n",
    "    \"\"\"Get enhanced transforms with proper normalization\"\"\"\n",
    "    train_transforms = AdvancedTransforms.get_train_transforms(mean, std)\n",
    "    val_transforms = AdvancedTransforms.get_val_transforms(mean, std)\n",
    "    return train_transforms, val_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934124cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENHANCED RESNET ARCHITECTURE FOR MUSIC EMOTION RECOGNITION\n",
    "\n",
    "class AttentionModule(nn.Module):\n",
    "    \"\"\"Spatial attention mechanism for focusing on facial features\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels):\n",
    "        super(AttentionModule, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, 1, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attention = self.conv(x)\n",
    "        attention = self.sigmoid(attention)\n",
    "        return x * attention\n",
    "\n",
    "class MusicEmotionResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet-18 architecture optimized for music emotion recognition.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=5, pretrained=True, dropout_rate=0.4):\n",
    "        super(MusicEmotionResNet, self).__init__()\n",
    "        \n",
    "        # Load pretrained ResNet-18\n",
    "        self.resnet = models.resnet18(pretrained=pretrained)\n",
    "        \n",
    "        # Modify first layer for grayscale input (1 channel instead of 3)\n",
    "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        \n",
    "        # Add attention mechanism after the first few layers\n",
    "        self.attention = AttentionModule(64)\n",
    "        \n",
    "        # Enhanced classifier head\n",
    "        num_features = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate * 0.7),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate * 0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize the new conv1 layer properly\n",
    "        nn.init.kaiming_normal_(self.resnet.conv1.weight, mode='fan_out', nonlinearity='relu')\n",
    "        \n",
    "        # Initialize classifier layers\n",
    "        self._initialize_classifier()\n",
    "    \n",
    "    def _initialize_classifier(self):\n",
    "        \"\"\"Initialize classifier layers with proper weights\"\"\"\n",
    "        for module in self.resnet.fc.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "            elif isinstance(module, nn.BatchNorm1d):\n",
    "                nn.init.constant_(module.weight, 1)\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Extract features through ResNet backbone\n",
    "        x = self.resnet.conv1(x)\n",
    "        x = self.resnet.bn1(x)\n",
    "        x = self.resnet.relu(x)\n",
    "        \n",
    "        # Apply attention mechanism early in the network\n",
    "        x = self.attention(x)\n",
    "        \n",
    "        x = self.resnet.maxpool(x)\n",
    "        x = self.resnet.layer1(x)\n",
    "        x = self.resnet.layer2(x)\n",
    "        x = self.resnet.layer3(x)\n",
    "        x = self.resnet.layer4(x)\n",
    "        \n",
    "        # Final classification\n",
    "        x = self.resnet.fc(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def get_attention_weights(self, x):\n",
    "        \"\"\"Extract attention weights for visualization\"\"\"\n",
    "        x = self.resnet.conv1(x)\n",
    "        x = self.resnet.bn1(x)\n",
    "        x = self.resnet.relu(x)\n",
    "        \n",
    "        attention_weights = self.attention.conv(x)\n",
    "        attention_weights = self.attention.sigmoid(attention_weights)\n",
    "        \n",
    "        return attention_weights\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss for handling class imbalance\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9f7a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENHANCED VISUALIZATION AND ANALYSIS FUNCTIONS\n",
    "\n",
    "def visualize_sample_images(dataset, num_samples=20):\n",
    "    \"\"\"Enhanced visualization of sample images with emotion-music mapping info\"\"\"\n",
    "    emotions = dataset.label_encoder.classes_\n",
    "    n_emotions = len(emotions)\n",
    "    samples_per_emotion = num_samples // n_emotions\n",
    "    \n",
    "    fig, axes = plt.subplots(n_emotions, samples_per_emotion, \n",
    "                            figsize=(samples_per_emotion * 2, n_emotions * 2))\n",
    "    \n",
    "    if n_emotions == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    elif samples_per_emotion == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    \n",
    "    # Color mapping for emotions\n",
    "    colors = {'angry': 'red', 'happy': 'gold', 'neutral': 'gray', \n",
    "              'sad': 'blue', 'surprise': 'orange'}\n",
    "    \n",
    "    print(\"Sample Images with Music Genre Mapping:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for i, emotion in enumerate(emotions):\n",
    "        # Get indices for this emotion\n",
    "        emotion_indices = [idx for idx, label in enumerate(dataset.labels) \n",
    "                          if dataset.label_encoder.classes_[label] == emotion]\n",
    "        \n",
    "        # Sample random images\n",
    "        sample_indices = np.random.choice(emotion_indices, \n",
    "                                        min(samples_per_emotion, len(emotion_indices)), \n",
    "                                        replace=False)\n",
    "        \n",
    "        # Display info\n",
    "        music_genres = ', '.join(CONFIG['emotion_to_music'][emotion][:3])\n",
    "        print(f\"{emotion:>8}: {music_genres}\")\n",
    "        \n",
    "        for j, idx in enumerate(sample_indices):\n",
    "            img_path = dataset.data[idx]\n",
    "            try:\n",
    "                image = Image.open(img_path)\n",
    "                if image.mode != 'L':\n",
    "                    image = image.convert('L')\n",
    "                \n",
    "                if samples_per_emotion == 1:\n",
    "                    ax = axes[i]\n",
    "                else:\n",
    "                    ax = axes[i, j]\n",
    "                \n",
    "                ax.imshow(image, cmap='gray')\n",
    "                ax.axis('off')\n",
    "                \n",
    "                if j == 0:\n",
    "                    ax.set_ylabel(emotion.upper(), fontsize=12, fontweight='bold',\n",
    "                                color=colors.get(emotion, 'black'))\n",
    "                \n",
    "                # Add music genre info as title for first image\n",
    "                if j == 0:\n",
    "                    genre_text = ' | '.join(CONFIG['emotion_to_music'][emotion][:2])\n",
    "                    ax.set_title(genre_text, fontsize=8, style='italic')\n",
    "                \n",
    "            except Exception as e:\n",
    "                if samples_per_emotion == 1:\n",
    "                    ax = axes[i]\n",
    "                else:\n",
    "                    ax = axes[i, j]\n",
    "                ax.text(0.5, 0.5, 'Error', ha='center', va='center')\n",
    "                ax.axis('off')\n",
    "    \n",
    "    plt.suptitle('Music-Relevant Emotion Samples with Genre Mapping', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CONFIG['output_dir'], 'music_emotion_samples.png'), \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_comprehensive_analysis(dataset):\n",
    "    \"\"\"Create comprehensive analysis plots\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # 1. Class distribution\n",
    "    class_counts = Counter(dataset.labels)\n",
    "    emotions = [dataset.label_encoder.classes_[i] for i in range(len(dataset.label_encoder.classes_))]\n",
    "    counts = [class_counts[i] for i in range(len(dataset.label_encoder.classes_))]\n",
    "    colors = ['#ff4444', '#44ff44', '#888888', '#4444ff', '#ffaa44']\n",
    "    \n",
    "    bars = axes[0,0].bar(emotions, counts, color=colors, alpha=0.8)\n",
    "    axes[0,0].set_title('Music-Relevant Emotion Distribution', fontweight='bold')\n",
    "    axes[0,0].set_ylabel('Number of Images')\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels and percentages\n",
    "    total = sum(counts)\n",
    "    for bar, count in zip(bars, counts):\n",
    "        height = bar.get_height()\n",
    "        percentage = (count / total) * 100\n",
    "        axes[0,0].text(bar.get_x() + bar.get_width()/2, height + 50,\n",
    "                      f'{count}\\\\n({percentage:.1f}%)', \n",
    "                      ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 2. Class imbalance visualization\n",
    "    max_count = max(counts)\n",
    "    min_count = min(counts)\n",
    "    imbalance_ratio = max_count / min_count\n",
    "    \n",
    "    normalized_counts = [c / max_count for c in counts]\n",
    "    axes[0,1].bar(emotions, normalized_counts, color=colors, alpha=0.8)\n",
    "    axes[0,1].set_title(f'Class Balance (Ratio: {imbalance_ratio:.2f})', fontweight='bold')\n",
    "    axes[0,1].set_ylabel('Normalized Count')\n",
    "    axes[0,1].tick_params(axis='x', rotation=45)\n",
    "    axes[0,1].axhline(y=0.5, color='red', linestyle='--', alpha=0.7, label='50% line')\n",
    "    axes[0,1].legend()\n",
    "    \n",
    "    # 3. Music genre mapping pie chart\n",
    "    genre_counts = defaultdict(int)\n",
    "    for emotion in emotions:\n",
    "        for genre in CONFIG['emotion_to_music'][emotion]:\n",
    "            genre_counts[genre] += class_counts[dataset.label_encoder.transform([emotion])[0]]\n",
    "    \n",
    "    top_genres = sorted(genre_counts.items(), key=lambda x: x[1], reverse=True)[:8]\n",
    "    genre_names = [g[0] for g in top_genres]\n",
    "    genre_values = [g[1] for g in top_genres]\n",
    "    \n",
    "    axes[0,2].pie(genre_values, labels=genre_names, autopct='%1.1f%%', startangle=90)\n",
    "    axes[0,2].set_title('Music Genre Distribution', fontweight='bold')\n",
    "    \n",
    "    # 4. Emotion-Music mapping network\n",
    "    axes[1,0].axis('off')\n",
    "    mapping_text = \"Emotion -> Music Genre Mapping:\\\\n\\\\n\"\n",
    "    for emotion in emotions:\n",
    "        genres = ', '.join(CONFIG['emotion_to_music'][emotion])\n",
    "        mapping_text += f\"{emotion.upper():>8}: {genres}\\\\n\"\n",
    "    \n",
    "    axes[1,0].text(0.1, 0.9, mapping_text, transform=axes[1,0].transAxes,\n",
    "                  fontsize=11, verticalalignment='top', fontfamily='monospace',\n",
    "                  bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "    \n",
    "    # 5. Training recommendations\n",
    "    axes[1,1].axis('off')\n",
    "    recommendations = f\"\"\"\n",
    "Training Recommendations:\n",
    "\n",
    "Class Imbalance:\n",
    "- Ratio: {imbalance_ratio:.2f}\n",
    "- Use weighted loss function\n",
    "- Apply focal loss for hard examples\n",
    "- Implement balanced sampling\n",
    "\n",
    "Data Augmentation:\n",
    "- Rotation: ±{CONFIG['augmentation']['rotation_degrees']}°\n",
    "- Horizontal flip: {CONFIG['augmentation']['horizontal_flip_prob']*100}%\n",
    "- Brightness/contrast: ±{CONFIG['augmentation']['brightness_factor']*100}%\n",
    "- Random erasing for robustness\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    axes[1,1].text(0.05, 0.95, recommendations, transform=axes[1,1].transAxes,\n",
    "                  fontsize=9, verticalalignment='top',\n",
    "                  bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))\n",
    "    \n",
    "    # 6. Model architecture diagram\n",
    "    axes[1,2].axis('off')\n",
    "    \n",
    "    axes[1,2].text(0.05, 0.95, transform=axes[1,2].transAxes,\n",
    "                  fontsize=9, verticalalignment='top', fontfamily='monospace',\n",
    "                  bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CONFIG['output_dir'], 'comprehensive_analysis.png'), \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return imbalance_ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e865adac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_enhanced_data_loaders():\n",
    "    \"\"\"Create optimized data loaders with advanced sampling strategies\"\"\"\n",
    "    \n",
    "    print(\"Creating enhanced data loaders for music emotion recognition...\")\n",
    "    \n",
    "    # Load dataset with music-focused preprocessing\n",
    "    full_dataset = MusicEmotionDataset(\n",
    "        CONFIG['dataset_root'], \n",
    "        apply_clahe=True, \n",
    "        filter_quality=True\n",
    "    )\n",
    "    \n",
    "    # Calculate dataset statistics for normalization\n",
    "    mean, std = calculate_dataset_statistics(full_dataset)\n",
    "    \n",
    "    # Create stratified data splits to maintain class balance\n",
    "    sss = StratifiedShuffleSplit(\n",
    "        n_splits=1, \n",
    "        test_size=CONFIG['validation_split'] + CONFIG['test_split'],\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    train_idx, temp_idx = next(sss.split(\n",
    "        np.zeros(len(full_dataset)), \n",
    "        full_dataset.labels\n",
    "    ))\n",
    "    \n",
    "    # Further split temp into validation and test\n",
    "    sss_val = StratifiedShuffleSplit(\n",
    "        n_splits=1,\n",
    "        test_size=CONFIG['test_split'] / (CONFIG['validation_split'] + CONFIG['test_split']),\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    val_idx, test_idx = next(sss_val.split(\n",
    "        np.zeros(len(temp_idx)),\n",
    "        [full_dataset.labels[i] for i in temp_idx]\n",
    "    ))\n",
    "    \n",
    "    # Convert to absolute indices\n",
    "    val_idx = temp_idx[val_idx]\n",
    "    test_idx = temp_idx[test_idx]\n",
    "    \n",
    "    print(f\"Data splits:\")\n",
    "    print(f\"  Training: {len(train_idx)} samples\")\n",
    "    print(f\"  Validation: {len(val_idx)} samples\") \n",
    "    print(f\"  Test: {len(test_idx)} samples\")\n",
    "    \n",
    "    # Get enhanced transforms\n",
    "    train_transforms, val_transforms = get_transforms(mean, std)\n",
    "    \n",
    "    # Create transformed datasets\n",
    "    train_dataset = TransformedDataset(full_dataset, train_idx, train_transforms)\n",
    "    val_dataset = TransformedDataset(full_dataset, val_idx, val_transforms)\n",
    "    test_dataset = TransformedDataset(full_dataset, test_idx, val_transforms)\n",
    "    \n",
    "    # Create weighted sampler for balanced training\n",
    "    weighted_sampler = full_dataset.get_weighted_sampler()\n",
    "    train_sampler_indices = [i for i, idx in enumerate(range(len(full_dataset))) if idx in train_idx]\n",
    "    train_weights = [weighted_sampler.weights[i] for i in train_idx]\n",
    "    train_sampler = WeightedRandomSampler(train_weights, len(train_weights))\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        sampler=train_sampler,  # Use weighted sampler instead of shuffle\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "        drop_last=True  # For stable batch norm\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(\"Enhanced data loaders created successfully\")\n",
    "    return train_loader, val_loader, test_loader, full_dataset, mean, std\n",
    "\n",
    "def setup_enhanced_model_and_training():\n",
    "    \"\"\"Setup enhanced model with training components\"\"\"\n",
    "    \n",
    "    # Device setup\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Initialize enhanced model\n",
    "    model = MusicEmotionResNet(\n",
    "        num_classes=len(CONFIG['music_emotions']),\n",
    "        pretrained=True,\n",
    "        dropout_rate=0.4\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    \n",
    "    return model, device\n",
    "\n",
    "def save_enhanced_preprocessing_info(dataset, train_size, val_size, test_size, \n",
    "                                   mean, std, class_weights, imbalance_ratio):\n",
    "    \"\"\"Save comprehensive preprocessing information\"\"\"\n",
    "    \n",
    "    preprocessing_info = {\n",
    "        'dataset_info': {\n",
    "            'total_size': len(dataset),\n",
    "            'train_size': train_size,\n",
    "            'val_size': val_size,\n",
    "            'test_size': test_size,\n",
    "            'num_classes': len(CONFIG['music_emotions']),\n",
    "            'original_emotions': CONFIG['original_emotions'],\n",
    "            'music_emotions': CONFIG['music_emotions'],\n",
    "            'excluded_emotions': list(set(CONFIG['original_emotions']) - set(CONFIG['music_emotions'])),\n",
    "            'class_imbalance_ratio': float(imbalance_ratio)\n",
    "        },\n",
    "        'preprocessing_settings': {\n",
    "            'image_size': CONFIG['image_size'],\n",
    "            'batch_size': CONFIG['batch_size'],\n",
    "            'quality_filtering': True,\n",
    "            'clahe_enhancement': True,\n",
    "            'advanced_augmentation': True\n",
    "        },\n",
    "        'dataset_statistics': {\n",
    "            'pixel_mean': float(mean),\n",
    "            'pixel_std': float(std),\n",
    "            'normalization_applied': True\n",
    "        },\n",
    "        'class_distribution': {\n",
    "            emotion: int(Counter(dataset.labels)[i]) \n",
    "            for i, emotion in enumerate(dataset.label_encoder.classes_)\n",
    "        },\n",
    "        'class_weights': class_weights.tolist(),\n",
    "        'emotion_music_mapping': CONFIG['emotion_to_music'],\n",
    "        'augmentation_config': CONFIG['augmentation'],\n",
    "        'model_config': {\n",
    "            'architecture': 'Enhanced ResNet-18',\n",
    "            'attention_mechanism': True,\n",
    "            'dropout_rate': 0.4,\n",
    "            'loss_function': 'Focal Loss',\n",
    "            'optimizer': 'AdamW'\n",
    "        },\n",
    "        'training_config': {\n",
    "            'num_epochs': CONFIG['num_epochs'],\n",
    "            'learning_rate': CONFIG['learning_rate'],\n",
    "            'weight_decay': CONFIG['weight_decay'],\n",
    "            'lr_scheduler': 'ReduceLROnPlateau',\n",
    "            'early_stopping': True,\n",
    "            'patience': CONFIG['patience']\n",
    "        },\n",
    "        'timestamp': pd.Timestamp.now().isoformat(),\n",
    "        'version': '2.0_music_focused'\n",
    "    }\n",
    "    \n",
    "    # Save to JSON\n",
    "    output_file = os.path.join(CONFIG['output_dir'], 'enhanced_preprocessing_info.json')\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(preprocessing_info, f, indent=2)\n",
    "    \n",
    "    print(f\"Enhanced preprocessing info saved to: {output_file}\")\n",
    "    \n",
    "    # Create detailed summary report\n",
    "    summary_report = f\"\"\"\n",
    "ENHANCED EMOTION DETECTION PREPROCESSING REPORT\n",
    "==============================================\n",
    "\n",
    "DATASET OVERVIEW:\n",
    "- Total Images: {len(dataset):,}\n",
    "- Music-Relevant Emotions: {len(CONFIG['music_emotions'])}\n",
    "- Excluded Emotions: {len(set(CONFIG['original_emotions']) - set(CONFIG['music_emotions']))}\n",
    "- Class Imbalance Ratio: {imbalance_ratio:.2f}\n",
    "\n",
    "DATA SPLITS:\n",
    "- Training: {train_size:,} samples ({train_size/len(dataset)*100:.1f}%)\n",
    "- Validation: {val_size:,} samples ({val_size/len(dataset)*100:.1f}%)\n",
    "- Test: {test_size:,} samples ({test_size/len(dataset)*100:.1f}%)\n",
    "\n",
    "DATASET STATISTICS:\n",
    "- Pixel Mean: {mean:.4f}\n",
    "- Pixel Std: {std:.4f}\n",
    "- Image Size: {CONFIG['image_size'][0]}x{CONFIG['image_size'][1]}\n",
    "- Batch Size: {CONFIG['batch_size']}\n",
    "\n",
    "MODEL ARCHITECTURE:\n",
    "- Base: Enhanced ResNet-18\n",
    "- Input: Grayscale (1 channel)\n",
    "- Attention: Spatial attention module\n",
    "- Classifier: Multi-layer with dropout\n",
    "- Output: {len(CONFIG['music_emotions'])} emotion classes\n",
    "\n",
    "MUSIC EMOTION MAPPING:\n",
    "{chr(10).join([f\"- {emotion}: {', '.join(genres)}\" for emotion, genres in CONFIG['emotion_to_music'].items()])}\n",
    "\n",
    "TRAINING OPTIMIZATIONS:\n",
    "- Loss Function: Focal Loss (handles imbalance)\n",
    "- Optimizer: AdamW with weight decay\n",
    "- Learning Rate: {CONFIG['learning_rate']} with scheduler\n",
    "- Early Stopping: {CONFIG['patience']} epochs patience\n",
    "- Regularization: Dropout + BatchNorm\n",
    "\n",
    "OUTPUT LOCATION: {CONFIG['output_dir']}\n",
    "MODEL SAVE LOCATION: {CONFIG['model_save_dir']}\n",
    "    \"\"\"\n",
    "    \n",
    "    summary_file = os.path.join(CONFIG['output_dir'], 'enhanced_preprocessing_summary.txt')\n",
    "    with open(summary_file, 'w') as f:\n",
    "        f.write(summary_report)\n",
    "    \n",
    "    print(f\"Detailed summary saved to: {summary_file}\")\n",
    "    return preprocessing_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7616012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_processed_dataset():\n",
    "    \"\"\"Create processed dataset and save to processed directory\"\"\"\n",
    "    \n",
    "    print(\"Creating processed dataset for music emotion recognition...\")\n",
    "    \n",
    "    # Create enhanced dataset with processing and copying\n",
    "    dataset = MusicEmotionDataset(\n",
    "        CONFIG['dataset_root'], \n",
    "        apply_clahe=True, \n",
    "        filter_quality=True,\n",
    "        copy_to_processed=True,\n",
    "        processed_dir=CONFIG['output_dir']\n",
    "    )\n",
    "    \n",
    "    # Calculate dataset statistics\n",
    "    mean, std = calculate_dataset_statistics(dataset)\n",
    "    \n",
    "    # Get class weights and statistics\n",
    "    class_weights = dataset.get_class_weights()\n",
    "    stats = dataset.get_statistics()\n",
    "    \n",
    "    # Save dataset information\n",
    "    dataset_info = {\n",
    "        'statistics': stats,\n",
    "        'class_weights': class_weights.tolist(),\n",
    "        'normalization': {'mean': float(mean), 'std': float(std)},\n",
    "        'config': CONFIG,\n",
    "        'processed_location': CONFIG['output_dir']\n",
    "    }\n",
    "    \n",
    "    # Save dataset info\n",
    "    info_file = os.path.join(CONFIG['output_dir'], 'dataset_info.json')\n",
    "    with open(info_file, 'w') as f:\n",
    "        json.dump(dataset_info, f, indent=2)\n",
    "    \n",
    "    print(f\"Dataset processed and saved to: {CONFIG['output_dir']}\")\n",
    "    print(f\"Total samples: {stats['total_samples']}\")\n",
    "    print(f\"Classes: {stats['emotions']}\")\n",
    "    print(f\"Class distribution: {stats['class_distribution']}\")\n",
    "    \n",
    "    return dataset, mean, std, class_weights\n",
    "\n",
    "def create_data_loaders_from_processed():\n",
    "    \"\"\"Create data loaders from processed dataset\"\"\"\n",
    "    \n",
    "    print(\"Creating data loaders from processed dataset...\")\n",
    "    \n",
    "    # Load from processed directory\n",
    "    processed_dataset = MusicEmotionDataset(\n",
    "        CONFIG['output_dir'],  # Use processed directory\n",
    "        apply_clahe=False,     # Already processed\n",
    "        filter_quality=False,  # Already filtered\n",
    "        copy_to_processed=False\n",
    "    )\n",
    "    \n",
    "    # Calculate or load statistics\n",
    "    mean, std = calculate_dataset_statistics(processed_dataset)\n",
    "    \n",
    "    # Create stratified splits\n",
    "    sss = StratifiedShuffleSplit(\n",
    "        n_splits=1, \n",
    "        test_size=CONFIG['validation_split'] + CONFIG['test_split'],\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    train_idx, temp_idx = next(sss.split(\n",
    "        np.zeros(len(processed_dataset)), \n",
    "        processed_dataset.labels\n",
    "    ))\n",
    "    \n",
    "    # Further split for validation and test\n",
    "    sss_val = StratifiedShuffleSplit(\n",
    "        n_splits=1,\n",
    "        test_size=CONFIG['test_split'] / (CONFIG['validation_split'] + CONFIG['test_split']),\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    val_idx, test_idx = next(sss_val.split(\n",
    "        np.zeros(len(temp_idx)),\n",
    "        [processed_dataset.labels[i] for i in temp_idx]\n",
    "    ))\n",
    "    \n",
    "    val_idx = temp_idx[val_idx]\n",
    "    test_idx = temp_idx[test_idx]\n",
    "    \n",
    "    # Get transforms\n",
    "    train_transforms, val_transforms = get_transforms(mean, std)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = TransformedDataset(processed_dataset, train_idx, train_transforms)\n",
    "    val_dataset = TransformedDataset(processed_dataset, val_idx, val_transforms)\n",
    "    test_dataset = TransformedDataset(processed_dataset, test_idx, val_transforms)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Data loaders created:\")\n",
    "    print(f\"  Training: {len(train_dataset)} samples\")\n",
    "    print(f\"  Validation: {len(val_dataset)} samples\")\n",
    "    print(f\"  Test: {len(test_dataset)} samples\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, processed_dataset\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main preprocessing pipeline\"\"\"\n",
    "    \n",
    "    print(\"Music Emotion Detection - Data Preprocessing\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Step 1: Create processed dataset\n",
    "    dataset, mean, std, class_weights = create_processed_dataset()\n",
    "    \n",
    "    # Step 2: Create data loaders\n",
    "    train_loader, val_loader, test_loader, processed_dataset = create_data_loaders_from_processed()\n",
    "    \n",
    "    # Step 3: Setup model\n",
    "    model, device = setup_enhanced_model_and_training()\n",
    "    \n",
    "    print(\"\\nPreprocessing completed successfully!\")\n",
    "    print(f\"Processed data saved to: {CONFIG['output_dir']}\")\n",
    "    \n",
    "    return {\n",
    "        'train_loader': train_loader,\n",
    "        'val_loader': val_loader,\n",
    "        'test_loader': test_loader,\n",
    "        'model': model,\n",
    "        'device': device,\n",
    "        'class_weights': class_weights,\n",
    "        'dataset_stats': {'mean': mean, 'std': std},\n",
    "        'focal_loss': FocalLoss(alpha=1, gamma=2),\n",
    "        'processed_dataset': processed_dataset\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490c8ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Execute preprocessing pipeline\n",
    "    results = main()\n",
    "    \n",
    "    print(\"\\nPreprocessing completed! Ready for model training.\")\n",
    "    print(f\"Processed dataset location: {CONFIG['output_dir']}\")\n",
    "    \n",
    "    # Save training info for model training notebook\n",
    "    training_info = {\n",
    "        'processed_data_dir': CONFIG['output_dir'],\n",
    "        'dataset_stats': {\n",
    "            'mean': float(results['dataset_stats']['mean']),\n",
    "            'std': float(results['dataset_stats']['std'])\n",
    "        },\n",
    "        'class_weights': [float(w) for w in results['class_weights'].tolist()],\n",
    "        'config': CONFIG\n",
    "    }\n",
    "    \n",
    "    info_file = os.path.join(CONFIG['output_dir'], 'training_ready_info.json')\n",
    "    with open(info_file, 'w') as f:\n",
    "        json.dump(training_info, f, indent=2)\n",
    "    \n",
    "    print(f\"Training info saved to: {info_file}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
