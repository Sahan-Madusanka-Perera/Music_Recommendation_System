{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b19a7a5-9a70-409c-bdb8-02fafbb11526",
   "metadata": {},
   "source": [
    "# FER2013 preprocessing for MobileNetV2(GrayScale  48x48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f68db838-d33b-4727-a225-a9994cb7c1ab",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 20:38:33.612486: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-13 20:38:33.623156: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-13 20:38:33.779358: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-13 20:38:33.784830: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-13 20:38:36.150031: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.13.0\n",
      "OpenCV Version: 4.8.0\n"
     ]
    }
   ],
   "source": [
    "# imports with aliases\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# TensorFlow imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Print versions\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"OpenCV Version: {cv2.__version__}\")\n",
    "\n",
    "# Define constants\n",
    "IMG_SIZE = (48, 48)\n",
    "BATCH_SIZE = 64\n",
    "INPUT_SHAPE = (*IMG_SIZE, 1)  # Grayscale single channel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29bb8792-dfc0-4247-8e5e-b8657899dd75",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected classes: ['angry', 'happy', 'sad', 'surprise', 'neutral']\n",
      "Input shape: (48, 48, 1)\n"
     ]
    }
   ],
   "source": [
    "# Define base directory and paths\n",
    "BASE_DIR = '../../data/raw/fer2013/'\n",
    "TRAIN_DIR = os.path.join(BASE_DIR, 'train')\n",
    "TEST_DIR = os.path.join(BASE_DIR, 'test')\n",
    "\n",
    "#Update the classes\n",
    "SELECTED_CLASSES = [\"angry\", \"happy\", \"sad\", \"surprise\", \"neutral\"]\n",
    "NUM_CLASSES = len(SELECTED_CLASSES)\n",
    "\n",
    "print(f\"Selected classes: {SELECTED_CLASSES}\")\n",
    "print(f\"Input shape: {INPUT_SHAPE}\")\n",
    "\n",
    "# CLASS_NAMES = sorted(list(set(EMOTION_MAP.values())))\n",
    "# NUM_CLASSES = len(CLASS_NAMES)\n",
    "\n",
    "# print(f\"Classes after merging: {CLASS_NAMES}\")\n",
    "# print(f\"Number of classes: {NUM_CLASSES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab194211-b7f6-4e17-bfd9-a5d4e3ce271f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (24176, 48, 48, 1)\n",
      "Test data shape: (6043, 48, 48, 1)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_image(img_path):\n",
    "    \"\"\"\n",
    "    Basic preprocessing for FER2013 images:\n",
    "    - Load as grayscale\n",
    "    - Normalize to [0, 1]\n",
    "    - Add channel dimension\n",
    "    \"\"\"\n",
    "    # Load image (already grayscale 48x48)\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    # Normalize to [0, 1]\n",
    "    img_normalized = img.astype(np.float32) / 255.0\n",
    "    \n",
    "    # Add channel dimension for CNN: (48, 48) → (48, 48, 1)\n",
    "    img_final = np.expand_dims(img_normalized, axis=-1)\n",
    "    \n",
    "    return img_final\n",
    "\n",
    "def load_and_preprocess_data(directory, valid_classes):\n",
    "    \"\"\"\n",
    "    Load images and labels from directory, apply preprocessing\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for emotion in os.listdir(directory):\n",
    "        if emotion not in valid_classes:\n",
    "            continue\n",
    "        emotion_path = os.path.join(directory, emotion)\n",
    "        for img_file in os.listdir(emotion_path):\n",
    "            img_path = os.path.join(emotion_path, img_file)\n",
    "            img = preprocess_image(img_path)\n",
    "            images.append(img)\n",
    "            labels.append(emotion)\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "X_train, y_train = load_and_preprocess_data(TRAIN_DIR, SELECTED_CLASSES)\n",
    "X_test, y_test = load_and_preprocess_data(TEST_DIR, SELECTED_CLASSES)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a1267b7-0107-4abd-bbb6-2bd8c764bd0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class weights for handling imbalance:\n",
      "angry: 1.21\n",
      "happy: 0.67\n",
      "sad: 1.00\n",
      "surprise: 1.52\n",
      "neutral: 0.97\n"
     ]
    }
   ],
   "source": [
    "# Convert string labels to numerical labels\n",
    "label_to_index = {emotion: idx for idx, emotion in enumerate(SELECTED_CLASSES)}\n",
    "index_to_label = {idx: emotion for emotion, idx in label_to_index.items()}\n",
    "\n",
    "# Convert string labels to numerical\n",
    "y_train_num = np.array([label_to_index[label] for label in y_train])\n",
    "y_test_num = np.array([label_to_index[label] for label in y_test])\n",
    "\n",
    "# One-hot encode labels\n",
    "y_train_encoded = to_categorical(y_train_num, num_classes=NUM_CLASSES)\n",
    "y_test_encoded = to_categorical(y_test_num, num_classes=NUM_CLASSES)\n",
    "\n",
    "# Calculate class weights for handling imbalance\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(y_train_num),\n",
    "    y=y_train_num\n",
    ")\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "print(\"\\nClass weights for handling imbalance:\")\n",
    "for idx, weight in class_weights.items():\n",
    "    print(f\"{index_to_label[idx]}: {weight:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19c204a0-7ad3-4b8e-a5fb-392ed0e642e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Augmented training samples:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x300 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create data augmentation generator\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=15,          # ±15 degrees\n",
    "    width_shift_range=0.1,      # ±10% horizontal shift\n",
    "    height_shift_range=0.1,     # ±10% vertical shift\n",
    "    brightness_range=[0.9, 1.1], # ±10% brightness\n",
    "    zoom_range=0.1,             # ±10% zoom\n",
    "    horizontal_flip=True,       # 50% chance (be careful with asymmetric emotions)\n",
    "    fill_mode='nearest'         # Fill missing pixels\n",
    ")\n",
    "\n",
    "# Validation/test generator (only normalization)\n",
    "test_datagen = ImageDataGenerator()\n",
    "\n",
    "# Create data generators\n",
    "train_generator = train_datagen.flow(\n",
    "    X_train,\n",
    "    y_train_encoded,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "validation_generator = test_datagen.flow(\n",
    "    X_test,\n",
    "    y_test_encoded,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Visualize augmented samples\n",
    "def plot_augmented_samples(generator, n_samples=5):\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    for i in range(n_samples):\n",
    "        # Get a batch of augmented images\n",
    "        for X_batch, y_batch in generator:\n",
    "            # Convert grayscale to RGB for display\n",
    "            img = np.squeeze(X_batch[0])\n",
    "            label = index_to_label[np.argmax(y_batch[0])]\n",
    "            \n",
    "            plt.subplot(1, n_samples, i+1)\n",
    "            plt.imshow(img, cmap='gray')\n",
    "            plt.title(label)\n",
    "            plt.axis('off')\n",
    "            break  \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nAugmented training samples:\")\n",
    "plot_augmented_samples(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6279c057-d17c-4d7d-a30f-9eb9fe240010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create directory for processed data\n",
    "PROCESSED_DIR = '../../data/processed/FC211042_Heshani'\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "# make sure X_train, y_train_encoded, index_to_label already exist\n",
    "train_out = Path(PROCESSED_DIR) / \"train\"\n",
    "test_out  = Path(PROCESSED_DIR) / \"test\"\n",
    "train_out.mkdir(parents=True, exist_ok=True)\n",
    "test_out.mkdir(parents=True,  exist_ok=True)\n",
    "\n",
    "def save_images(array, labels, root_dir):\n",
    "    for idx, (img, label_vec) in enumerate(zip(array, labels)):\n",
    "        class_name = index_to_label[int(np.argmax(label_vec))]\n",
    "        class_dir  = root_dir / class_name\n",
    "        class_dir.mkdir(exist_ok=True)\n",
    "        img_uint8  = (img.squeeze() * 255).astype(\"uint8\")      # back to 0‑255\n",
    "        file_path  = class_dir / f\"{idx:06d}.jpg\"\n",
    "        cv2.imwrite(str(file_path), img_uint8)\n",
    "\n",
    "# save train and test sets\n",
    "save_images(X_train, y_train_encoded, train_out)\n",
    "save_images(X_test,  y_test_encoded,  test_out)\n",
    "\n",
    "print(\"Preprocessed data saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4bcbb8-8a71-45ea-b3f3-494ed09665fa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
