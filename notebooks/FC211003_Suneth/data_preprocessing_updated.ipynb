{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64192187",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "016987ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from hashlib import md5\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475d615d",
   "metadata": {},
   "source": [
    "# config and storeage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0b6fb76-d551-49c2-b112-abe1c1feba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Config ---\n",
    "INPUT_DIR = \"../../data/raw/fer2013/train\"\n",
    "OUTPUT_FILE = \"../../data/processed/FC211003_Suneth/train.npz\"\n",
    "\n",
    "TARGET_SIZE = (48, 48)\n",
    "MAX_IMAGES_PER_CLASS = 4000       # Apply to all classes\n",
    "MIN_FACE_CONFIDENCE = 1.1\n",
    "MIN_NEIGHBORS = 4\n",
    "\n",
    "# Final class map\n",
    "MERGE_MAP = {\n",
    "    \"happy\": \"happy\",\n",
    "    \"sad\": \"sad\",\n",
    "    \"angry\": \"stressed\",      # Merge angry â†’ stressed\n",
    "    \"disgust\": None,          # Remove\n",
    "    \"fear\": \"fear\",\n",
    "    \"neutral\": \"neutral\",\n",
    "    \"surprise\": None          # Remove surprise\n",
    "}\n",
    "\n",
    "seen_hashes = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921f7a17-5c6b-4bbf-aafc-fc0df8f4a3f4",
   "metadata": {},
   "source": [
    "## PreProcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcf5c7f-b8a8-4f5c-97d8-9ba7bdfd3338",
   "metadata": {},
   "source": [
    "### Step 1 - Dataset Scanning Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87d95b19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "466519a700dc4203b9361de52881d6c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classes:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "angry:   0%|          | 0/3995 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SKIP] Class 'disgust' removed as per EDA.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "fear:   0%|          | 0/4097 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "happy:   0%|          | 0/7215 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "neutral:   0%|          | 0/4965 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sad:   0%|          | 0/4830 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SKIP] Class 'surprise' removed as per EDA.\n"
     ]
    }
   ],
   "source": [
    "# --- Class image counter ---\n",
    "class_image_counts = {}\n",
    "\n",
    "# Loop through dataset class folders\n",
    "all_files = []\n",
    "\n",
    "for original_class in tqdm(os.listdir(INPUT_DIR), desc=\"Classes\"):\n",
    "    class_path = os.path.join(INPUT_DIR, original_class)\n",
    "    if not os.path.isdir(class_path):\n",
    "        continue\n",
    "\n",
    "    mapped_class = MERGE_MAP.get(original_class.lower())\n",
    "    if mapped_class is None:\n",
    "        print(f\"[SKIP] Class '{original_class}' removed as per EDA.\")\n",
    "        continue\n",
    "\n",
    "    for file_name in tqdm(os.listdir(class_path), desc=f\"{original_class}\", leave=False):\n",
    "        fpath = os.path.join(class_path, file_name)\n",
    "\n",
    "        # Skip if class is already full\n",
    "        if class_image_counts.get(mapped_class, 0) >= MAX_IMAGES_PER_CLASS:\n",
    "            continue\n",
    "\n",
    "        all_files.append((fpath, mapped_class))\n",
    "        class_image_counts[mapped_class] = class_image_counts.get(mapped_class, 0) + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff22809-323b-46c4-b7a8-05edbd25e9d7",
   "metadata": {},
   "source": [
    "### Step 2 - Load Images, Remove Duplicates, and Filter Bad Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b519a147-278d-4a61-baeb-d04e1dbc47a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Starting image loading and filtering...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e772f5ddb23949b5a1583828293a0fa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Images:   0%|          | 0/19995 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"ðŸ“¦ Starting image loading and filtering...\")\n",
    "\n",
    "for fpath, mapped_class in tqdm(all_files, desc=\"Processing Images\"):\n",
    "    try:\n",
    "        # Step 1: Remove duplicate images\n",
    "        with open(fpath, 'rb') as f:\n",
    "            img_hash = md5(f.read()).hexdigest()\n",
    "        if img_hash in seen_hashes:\n",
    "            continue\n",
    "        seen_hashes.add(img_hash)\n",
    "\n",
    "        # Step 2: Load image and convert to grayscale\n",
    "        img = Image.open(fpath).convert(\"L\")\n",
    "        img_np = np.array(img)\n",
    "\n",
    "        # Step 3: Skip blank or too small images\n",
    "        if img_np.shape[0] < 32 or img_np.shape[1] < 32 or np.mean(img_np) < 5:\n",
    "            continue\n",
    "\n",
    "        # Step 4: Face detection\n",
    "        faces = face_cascade.detectMultiScale(\n",
    "            img_np, scaleFactor=MIN_FACE_CONFIDENCE, minNeighbors=MIN_NEIGHBORS\n",
    "        )\n",
    "        if len(faces) == 0:\n",
    "            continue  # Skip if no face detected\n",
    "\n",
    "        # Step 5: Resize image\n",
    "        img_resized = Image.fromarray(img_np).resize(TARGET_SIZE, Image.BILINEAR)\n",
    "\n",
    "        # Step 6: Normalize pixels\n",
    "        img_array = np.array(img_resized, dtype=np.float32) / 255.0\n",
    "        img_array = np.expand_dims(img_array, axis=-1)  # shape: (48, 48, 1)\n",
    "\n",
    "        # Step 7: Save to final arrays\n",
    "        images.append(img_array)\n",
    "        labels.append(mapped_class)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed on {fpath}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d783caa8",
   "metadata": {},
   "source": [
    "### Step 3 - Load Images, Remove Duplicates, and Filter Bad Samples\n",
    "output saving as a single.npz file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e8e42fb-b214-462c-8eaa-100402cb0053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¤ Encoding labels...\n",
      "ðŸ’¾ Saving 5692 samples to ../../data/processed/FC211003_Suneth/train.npz...\n",
      "âœ… Dataset saved successfully!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "print(\"ðŸ”¤ Encoding labels...\")\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(labels)            # e.g., 'happy' â†’ 0\n",
    "y_onehot = to_categorical(y_encoded)            # one-hot encode: [0, 1, 0, 0, 0, 0]\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = np.array(images, dtype=np.float32)\n",
    "y = np.array(y_onehot, dtype=np.float32)\n",
    "\n",
    "# Save to disk\n",
    "print(f\"ðŸ’¾ Saving {X.shape[0]} samples to {OUTPUT_FILE}...\")\n",
    "np.savez_compressed(OUTPUT_FILE, X=X, y=y, label_names=le.classes_)\n",
    "print(\"âœ… Dataset saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6707fe8-4ae2-4966-968f-6968b6351cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
