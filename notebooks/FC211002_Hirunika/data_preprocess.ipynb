{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f6f78b6-2983-4f8f-893a-4365e4a1db17",
   "metadata": {},
   "source": [
    "                      Data Preprocessing\n",
    "\n",
    "\n",
    "        \n",
    "        Optional Augmentations: \n",
    "            -Rotation: ±15 degrees (faces can be slightly tilted)\n",
    "            -Horizontal flip: ONLY for non-asymmetric emotions\n",
    "            -Brightness adjustment: ±10% (lighting variations)\n",
    "            -Small zoom: ±5% (distance variations)\n",
    "            -NO vertical flip (would create upside-down faces!)\n",
    "\n",
    "        Steps:\n",
    "    \n",
    "        \n",
    "       \n",
    "\n",
    "Total corrupted files found: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a828278a-f8fa-4be0-a787-e4b72ab4034c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "#from tqdm import tqdm\n",
    "# import shutil\n",
    "import random\n",
    "from pathlib import Path\n",
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503c1231-467d-4f32-9420-89e2d31fd663",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set paths\n",
    "RAW_DIR = Path(\"/app/data/raw/fer2013\")\n",
    "PROCESSED_DIR = Path(\"/app/data/processed/FC211002_Hirunika\")\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336e7a6b-3720-4fa8-8827-7883c8f0e3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mapping FER2013 classes to 5 project classes\n",
    "CLASS_MAPPING = {\n",
    "    'angry': 'angry',\n",
    "    'disgust': 'angry',\n",
    "    'fear': 'stressed',\n",
    "    'surprise': 'stressed',\n",
    "    'happy': 'happy',\n",
    "    'neutral': 'neutral',\n",
    "    'sad': 'sad'\n",
    "}\n",
    "TARGET_CLASSES = ['angry', 'happy', 'sad', 'stressed', 'neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964394de-4a5d-4f6f-b057-516bd0aa8c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Preprocessing \n",
    "\n",
    "def preprocess_image(img_path):\n",
    "    img = cv2.imread(str(img_path), cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None or img.shape != (48, 48):\n",
    "        return None\n",
    "    img = cv2.resize(img, (224, 224))                          # Resize\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)            # Convert to RGB\n",
    "    img_rgb = img_rgb.astype(np.float32) / 255.0               # Normalize to [0, 1]\n",
    "    return img_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893a0bb8-4b20-4c49-a5e9-60d8f7b7e584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "\n",
    "def augment_image(img):\n",
    "    rows, cols, _ = img.shape\n",
    "\n",
    "    angle = random.uniform(-15, 15)\n",
    "    M = cv2.getRotationMatrix2D((cols / 2, rows / 2), angle, 1)\n",
    "    img = cv2.warpAffine(img, M, (cols, rows), borderMode=cv2.BORDER_REFLECT)\n",
    "\n",
    "    factor = random.uniform(0.9, 1.1)\n",
    "    img = np.clip(img * factor, 0, 1)\n",
    "\n",
    "    zoom_factor = random.uniform(0.95, 1.05)\n",
    "    new_w = int(cols / zoom_factor)\n",
    "    new_h = int(rows / zoom_factor)\n",
    "    x1 = max((cols - new_w) // 2, 0)\n",
    "    y1 = max((rows - new_h) // 2, 0)\n",
    "    cropped = img[y1:y1+new_h, x1:x1+new_w]\n",
    "    img = cv2.resize(cropped, (224, 224))\n",
    "\n",
    "    if random.random() > 0.5:\n",
    "        img = cv2.flip(img, 1)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089cbbae-0447-4c8c-9f47-18084fde9666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_split(split, augment=False, augment_count=3):\n",
    "    print(f\"\\n Processing split: '{split}'\")\n",
    "    input_dir = RAW_DIR / split\n",
    "    output_dir = PROCESSED_DIR / split\n",
    "    total_images = 0\n",
    "    saved_images = 0\n",
    "    skipped_images = 0\n",
    "\n",
    "    for orig_class in os.listdir(input_dir):\n",
    "        orig_path = input_dir / orig_class\n",
    "        if not orig_path.is_dir():\n",
    "            continue\n",
    "\n",
    "        mapped_class = CLASS_MAPPING.get(orig_class)\n",
    "        if mapped_class not in TARGET_CLASSES:\n",
    "            print(f\" Skipping class '{orig_class}' — not in mapping\")\n",
    "            continue\n",
    "\n",
    "        out_class_dir = output_dir / mapped_class\n",
    "        out_class_dir.mkdir(parents=True, exist_ok=True)\n",
    "        image_files = os.listdir(orig_path)\n",
    "\n",
    "        print(f\" Class '{orig_class}' → '{mapped_class}': {len(image_files)} images\")\n",
    "\n",
    "        for img_name in tqdm(image_files, desc=f\"{orig_class} → {mapped_class}\"):\n",
    "            total_images += 1\n",
    "            img_path = orig_path / img_name\n",
    "            img = preprocess_image(img_path)\n",
    "            if img is None:\n",
    "                print(f\" Skipped '{img_name}' — corrupt or invalid format\")\n",
    "                skipped_images += 1\n",
    "                continue\n",
    "\n",
    "            save_name = Path(img_name).stem + \".png\"\n",
    "            save_path = out_class_dir / save_name\n",
    "            cv2.imwrite(str(save_path), (img * 255).astype(np.uint8))\n",
    "            saved_images += 1\n",
    "\n",
    "            if augment:\n",
    "                for i in range(augment_count):\n",
    "                    aug_img = augment_image(img.copy())\n",
    "                    aug_name = Path(img_name).stem + f\"_aug{i}.png\"\n",
    "                    aug_path = out_class_dir / aug_name\n",
    "                    cv2.imwrite(str(aug_path), (aug_img * 255).astype(np.uint8))\n",
    "                    saved_images += 1\n",
    "\n",
    "    print(f\"\\n    Finished split: '{split}'\")\n",
    "    print(f\"   ➤ Total images found     : {total_images}\")\n",
    "    print(f\"   ➤ Successfully processed : {saved_images}\")\n",
    "    print(f\"   ➤ Skipped (corrupt/etc)  : {skipped_images}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423fbfe7-e24f-4435-9ad7-59bd6e4f6c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e6584f-420e-41fa-bafb-74773f115918",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " # Run Script\n",
    "from tqdm import tqdm \n",
    "if __name__ == \"__main__\":\n",
    "    print(\" Starting FER2013 preprocessing for EfficientNetB0...\\n\")\n",
    "\n",
    "    process_split(\"train\", augment=True, augment_count=3)\n",
    "    process_split(\"test\", augment=False)\n",
    "\n",
    "    print(f\"\\n All preprocessing complete.\")\n",
    "    print(f\"Processed data saved at: {PROCESSED_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
