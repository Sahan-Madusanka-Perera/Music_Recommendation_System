{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f6f78b6-2983-4f8f-893a-4365e4a1db17",
   "metadata": {},
   "source": [
    "                       Data Preprocessing\n",
    "\n",
    "        Model: EfficientNet B0 \n",
    "\n",
    "        Already Done\n",
    "            -Images are already 48x48 pixels\n",
    "            -Images are already grayscale\n",
    "            -Face detection already applied (FER2013 is pre-cropped)\n",
    "\n",
    "\n",
    "            \n",
    "        Steps\n",
    "        -01]Load raw FER2013 data\n",
    "        -02]Map original class → project class\n",
    "        -03]Preprocess images (resize, normalize, RGB)\n",
    "        -04]Save processed images\n",
    "        -05]Apply data augmentation\n",
    "        -06]Organize all images by mapped class\n",
    "\n",
    "        \n",
    "    \n",
    "        \n",
    "       \n",
    "\n",
    "        Total corrupted files found: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a828278a-f8fa-4be0-a787-e4b72ab4034c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "503c1231-467d-4f32-9420-89e2d31fd663",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set paths\n",
    "RAW_DIR = Path(\"/app/data/raw/fer2013\")\n",
    "PROCESSED_DIR = Path(\"/app/data/processed/FC211002_Hirunika\")\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "336e7a6b-3720-4fa8-8827-7883c8f0e3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mapping FER2013 classes to 5 project classes\n",
    "CLASS_MAPPING = {\n",
    "    'angry': 'angry',\n",
    "    'disgust': 'angry',\n",
    "    'fear': 'stressed',\n",
    "    'surprise': 'stressed',\n",
    "    'happy': 'happy',\n",
    "    'neutral': 'neutral',\n",
    "    'sad': 'sad'\n",
    "}\n",
    "TARGET_CLASSES = ['angry', 'happy', 'sad', 'stressed', 'neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "964394de-4a5d-4f6f-b057-516bd0aa8c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Preprocessing \n",
    "\n",
    "def preprocess_image(img_path):\n",
    "    img = cv2.imread(str(img_path), cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None or img.shape != (48, 48):   #Check that the image is of shape (48, 48), else skip.\n",
    "        return None\n",
    "    img = cv2.resize(img, (224, 224))   # Resize\n",
    "    #img = cv2.resize(img, (48, 48))\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)            # Convert to RGB\n",
    "    img_rgb = img_rgb.astype(np.float32) / 255.0               # Normalize to [0, 1]\n",
    "    return img_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "893a0bb8-4b20-4c49-a5e9-60d8f7b7e584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "\n",
    "def augment_image(img):\n",
    "    rows, cols, _ = img.shape     # Get image dimensions\n",
    "\n",
    "    # ---------------------- Rotation ----------------------\n",
    "    # Randomly rotate the image between -15 and 15 degrees\n",
    "    angle = random.uniform(-15, 15)\n",
    "    angle = random.uniform(-15, 15)\n",
    "    M = cv2.getRotationMatrix2D((cols / 2, rows / 2), angle, 1)\n",
    "    img = cv2.warpAffine(img, M, (cols, rows), borderMode=cv2.BORDER_REFLECT)\n",
    "\n",
    "    # ---------------------- Brightness Adjustment ----------------------\n",
    "    # Randomly adjust brightness by multiplying pixel values by a factor between 0.9 and 1.1\n",
    "    factor = random.uniform(0.9, 1.1)\n",
    "    img = np.clip(img * factor, 0, 1)\n",
    "\n",
    "    # ---------------------- Zoom In/Out ----------------------\n",
    "    # Randomly zoom in or out by resizing the image to a smaller or larger size and then cropping/resizing\n",
    "    zoom_factor = random.uniform(0.95, 1.05)\n",
    "    new_w = int(cols / zoom_factor)\n",
    "    new_h = int(rows / zoom_factor)\n",
    "\n",
    "    # Calculate top-left corner for center cropping\n",
    "    x1 = max((cols - new_w) // 2, 0)\n",
    "    y1 = max((rows - new_h) // 2, 0)\n",
    "    cropped = img[y1:y1+new_h, x1:x1+new_w] # Crop the zoomed image\n",
    "    img = cv2.resize(cropped, (224, 224))\n",
    "\n",
    "\n",
    "    # ---------------------- Horizontal Flip ----------------------\n",
    "    # Randomly flip image horizontally (50% chance)\n",
    "    if random.random() > 0.5:\n",
    "        img = cv2.flip(img, 1)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "089cbbae-0447-4c8c-9f47-18084fde9666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_split(split, augment=False, augment_count=3):\n",
    "    print(f\"\\n Processing split: '{split}'\")\n",
    "\n",
    "    # Define input and output directories\n",
    "    input_dir = RAW_DIR / split\n",
    "    output_dir = PROCESSED_DIR / split\n",
    "    \n",
    "    total_images = 0\n",
    "    saved_images = 0\n",
    "    skipped_images = 0\n",
    "\n",
    "    # Loop through each original class in the input directory\n",
    "    for orig_class in os.listdir(input_dir):\n",
    "        orig_path = input_dir / orig_class\n",
    "        if not orig_path.is_dir():  # Skip if it's not a directory\n",
    "            continue\n",
    "\n",
    "        mapped_class = CLASS_MAPPING.get(orig_class) # Get the mapped class using the class mapping dictionary\n",
    "        if mapped_class not in TARGET_CLASSES:\n",
    "            print(f\" Skipping class '{orig_class}' — not in mapping\")\n",
    "            continue\n",
    "\n",
    "        out_class_dir = output_dir / mapped_class  # Create output directory for the mapped class\n",
    "        out_class_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        image_files = os.listdir(orig_path)# List all image files in the original class directory\n",
    "\n",
    "        print(f\" Class '{orig_class}' → '{mapped_class}': {len(image_files)} images\")\n",
    "\n",
    "        # Process each image file\n",
    "\n",
    "        for img_name in tqdm(image_files, desc=f\"{orig_class} → {mapped_class}\"):\n",
    "            total_images += 1\n",
    "            img_path = orig_path / img_name\n",
    "            img = preprocess_image(img_path) # Preprocess the image (resize, convert, normalize)\n",
    "\n",
    "            # Skip if image is invalid or corrupt\n",
    "            if img is None:\n",
    "                print(f\" Skipped '{img_name}' — corrupt or invalid format\")\n",
    "                skipped_images += 1\n",
    "                continue\n",
    "\n",
    "            save_name = Path(img_name).stem + \".png\"\n",
    "            save_path = out_class_dir / save_name\n",
    "            cv2.imwrite(str(save_path), (img * 255).astype(np.uint8))\n",
    "            saved_images += 1\n",
    "\n",
    "            # If augmentation is enabled, create and save augmented versions\n",
    "            if augment:\n",
    "                for i in range(augment_count):\n",
    "                    aug_img = augment_image(img.copy())\n",
    "                    aug_name = Path(img_name).stem + f\"_aug{i}.png\"\n",
    "                    aug_path = out_class_dir / aug_name\n",
    "                    cv2.imwrite(str(aug_path), (aug_img * 255).astype(np.uint8))\n",
    "                    saved_images += 1\n",
    "\n",
    "    # Print a summary of processing for the split\n",
    "    print(f\"\\n    Finished split: '{split}'\")\n",
    "    print(f\"   ➤ Total images found     : {total_images}\")\n",
    "    print(f\"   ➤ Successfully processed : {saved_images}\")\n",
    "    print(f\"   ➤ Skipped (corrupt/etc)  : {skipped_images}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "423fbfe7-e24f-4435-9ad7-59bd6e4f6c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/site-packages (4.67.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3e6584f-420e-41fa-bafb-74773f115918",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting FER2013 preprocessing for EfficientNetB0...\n",
      "\n",
      "\n",
      " Processing split: 'train'\n",
      " Class 'angry' → 'angry': 3995 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "angry → angry: 100%|██████████| 3995/3995 [09:36<00:00,  6.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Class 'disgust' → 'angry': 436 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "disgust → angry: 100%|██████████| 436/436 [01:10<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Class 'fear' → 'stressed': 4097 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fear → stressed: 100%|██████████| 4097/4097 [13:33<00:00,  5.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Class 'happy' → 'happy': 7215 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "happy → happy: 100%|██████████| 7215/7215 [21:43<00:00,  5.53it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Class 'neutral' → 'neutral': 4965 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "neutral → neutral: 100%|██████████| 4965/4965 [14:32<00:00,  5.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Class 'sad' → 'sad': 4830 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sad → sad: 100%|██████████| 4830/4830 [13:47<00:00,  5.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Class 'surprise' → 'stressed': 3171 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "surprise → stressed: 100%|██████████| 3171/3171 [08:46<00:00,  6.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Finished split: 'train'\n",
      "   ➤ Total images found     : 28709\n",
      "   ➤ Successfully processed : 114836\n",
      "   ➤ Skipped (corrupt/etc)  : 0\n",
      "\n",
      " Processing split: 'test'\n",
      " Class 'angry' → 'angry': 958 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "angry → angry: 100%|██████████| 958/958 [01:04<00:00, 14.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Class 'disgust' → 'angry': 111 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "disgust → angry: 100%|██████████| 111/111 [00:06<00:00, 16.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Class 'fear' → 'stressed': 1024 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fear → stressed: 100%|██████████| 1024/1024 [01:12<00:00, 14.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Class 'happy' → 'happy': 1774 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "happy → happy: 100%|██████████| 1774/1774 [02:03<00:00, 14.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Class 'neutral' → 'neutral': 1233 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "neutral → neutral: 100%|██████████| 1233/1233 [01:28<00:00, 13.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Class 'sad' → 'sad': 1247 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sad → sad: 100%|██████████| 1247/1247 [01:25<00:00, 14.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Class 'surprise' → 'stressed': 831 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "surprise → stressed: 100%|██████████| 831/831 [01:00<00:00, 13.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Finished split: 'test'\n",
      "   ➤ Total images found     : 7178\n",
      "   ➤ Successfully processed : 7178\n",
      "   ➤ Skipped (corrupt/etc)  : 0\n",
      "\n",
      " All preprocessing complete.\n",
      "Processed data saved at: /app/data/processed/FC211002_Hirunika\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    " # Run Script\n",
    "from tqdm import tqdm \n",
    "if __name__ == \"__main__\":\n",
    "    print(\" Starting FER2013 preprocessing for EfficientNetB0...\\n\")\n",
    "\n",
    "    process_split(\"train\", augment=True, augment_count=3)\n",
    "    process_split(\"test\", augment=False)\n",
    "\n",
    "    print(f\"\\n All preprocessing complete.\")\n",
    "    print(f\"Processed data saved at: {PROCESSED_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b15097f-7266-412a-930c-07207cc69ba7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
