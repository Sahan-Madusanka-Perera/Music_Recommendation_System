{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c239fc51-3fc5-4b05-9c78-31a15b5bf35d",
   "metadata": {},
   "source": [
    "# Data Preprocessing_EfficientNet B0\n",
    "\n",
    "    Already Done\r\n",
    "        -Images are already 48x48 pixels\r\n",
    "        -Images are already grayscale\r\n",
    "        -Face detection already applied (FER2013 is pre-cropp\n",
    "----------------------------------------------------------------------------\n",
    "        - 1.Load raw data directories for train and test splits.\n",
    "        - 2.For each image in the dataset:\n",
    "            -Read the image in grayscale.\n",
    "            - Check if image is corrupted or unreadable.\n",
    "            - Verify image size is exactly 48×48 pixels.\n",
    "            - Check if image is nearly blank (low standard deviation).\n",
    "            - Check if image is blurry (low variance of Laplacian).\n",
    "            - Remove duplicate images using SHA256 hash comparison.\n",
    "            - If all checks pass, normalize pixel values to [0, 1].\n",
    "        - 3.Map original FER2013 classes (7 classes) to your 5 target classes:\n",
    "        - 4.Split the training data into training and validation sets , stratified by class to maintain class proportions.\n",
    "        - 5.Encode labels:\n",
    "            - Convert string class labels to integers using LabelEncoder.\n",
    "            - One-hot encode the integer labels for model training.\n",
    "        - 6.Print class distribution counts for training, validation, and test sets to show class imbalance.\n",
    "        - 7.Compute class weights based on the training data to help balance the loss function during training.\n",
    "        - 8.Print final dataset shapes for all splits (train, val, test).\n",
    "        - 9.Save processed datasets (images and labels) as compressed .npz files (train.npz, val.npz, test.npz).ed)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d8d15a2-31f8-420c-a09d-f3b40b1405a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 04:39:45.915426: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-16 04:39:45.934045: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-16 04:39:46.092814: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-16 04:39:46.096669: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-16 04:39:47.321176: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7155e42a-8c77-46f2-8636-a0245632f118",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7487245a-3897-4b32-aa39-2b199d900078",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set paths\n",
    "RAW_DIR = Path(\"/app/data/raw/fer2013\")\n",
    "PROCESSED_DIR = Path(\"/app/data/processed/FC211002_Nethmi\")\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36b033db-766b-413a-a53d-f44992ef79ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mapping FER2013 classes to 5 project classes\n",
    "CLASS_MAPPING = {\n",
    "    'angry': 'angry',\n",
    "    'disgust': 'angry',\n",
    "    'fear': 'stressed',\n",
    "    'surprise': 'stressed',\n",
    "    'happy': 'happy',\n",
    "    'neutral': 'neutral',\n",
    "    'sad': 'sad'\n",
    "}\n",
    "TARGET_CLASSES = ['angry', 'happy', 'sad', 'stressed', 'neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79a8faeb-5f92-48a8-9932-0a7455b60608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thresholds for low-quality image detection\n",
    "BLANK_STD_THRESHOLD = 5      # std dev below means nearly blank\n",
    "BLUR_THRESHOLD = 100         # variance of Laplacian below means blurry\n",
    "VAL_SPLIT_RATIO = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dc1c83-456b-4d48-8ab7-baf7aa992ae9",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b304565f-190f-43f7-ac85-76465ea812ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img):\n",
    "    # Normalize a grayscale image to the [0,1] range.\n",
    "    # Input img - numpy array of shape (48, 48), dtype uint8\n",
    "    # Outp normalized float32 image of same shape with values between 0 & 1\n",
    "    return img.astype(np.float32) / 255.0  # shape: (48, 48)\n",
    "\n",
    "# Check if the image is essentially blank (very low pixel variance).\n",
    "def is_blank_image(img, threshold=BLANK_STD_THRESHOLD):\n",
    "    return np.std(img) < threshold\n",
    "    \n",
    " # Determine if an image is blurry using variance of Laplacian method.\n",
    "def is_blurry(img, threshold=BLUR_THRESHOLD):\n",
    "    return cv2.Laplacian(img, cv2.CV_64F).var() < threshold\n",
    "   \n",
    "# Compute SHA256 hash of an image file to detect duplicates\n",
    "def hash_image(img_path):\n",
    "    with open(img_path, 'rb') as f:\n",
    "        return hashlib.sha256(f.read()).hexdigest()\n",
    "    \n",
    "#Print count of samples per class for a dataset split.   \n",
    "def print_class_distribution(labels, label_encoder, split_name): \n",
    "    print(f\"\\n Class distribution in {split_name} set:\")\n",
    "    counts = Counter(labels)\n",
    "    for cls in label_encoder.classes_:\n",
    "        print(f\"  {cls}: {counts.get(cls, 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8424ea-c25b-43b7-ac85-29406bc7b1d4",
   "metadata": {},
   "source": [
    "##  Preprocessing Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e1316e-98bf-4268-be8f-c828b5202388",
   "metadata": {},
   "source": [
    "    - Steps include loading images, filtering by quality, mapping classes,\n",
    "    - removing duplicates, normalizing, and collecting labels.\n",
    "    \n",
    "    - split (str): Dataset split name, e.g., 'train' or 'test'.\n",
    "\n",
    "    - Returns:\n",
    "         - Tuple of numpy arrays: (images, labels)\n",
    "         - images: Array of preprocessed grayscale images (normalized)\n",
    "         - labels: Corresponding array of target class labels (strings)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d4f19dd-f0dd-4357-ac24-d04b1b802bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_split(split):\n",
    "    \n",
    "    print(f\"\\n Processing split: '{split}'\")\n",
    "    \n",
    "    # Define directory containing images for this split\n",
    "    input_dir = RAW_DIR / split\n",
    "\n",
    "    images = []          # List to hold processed image arrays\n",
    "    labels = []          # Corresponding list to hold class labels\n",
    "    seen_hashes = set()  # Set to track image hashes to avoid duplicates\n",
    "    skipped = 0          # Counter for skipped images due to filters\n",
    "    saved = 0            # Counter for successfully processed images\n",
    "\n",
    "    # Iterate over original classes in the split directory\n",
    "    for orig_class in os.listdir(input_dir):\n",
    "        class_dir = input_dir / orig_class\n",
    "        \n",
    "        # Skip if it's not a directory\n",
    "        if not class_dir.is_dir():\n",
    "            continue\n",
    "        \n",
    "        # Map original class to target class (e.g., 'disgust' → 'angry')\n",
    "        target_class = CLASS_MAPPING.get(orig_class)\n",
    "        \n",
    "        # Skip classes not in the defined target classes\n",
    "        if target_class not in TARGET_CLASSES:\n",
    "            continue\n",
    "        \n",
    "        # Iterate over all images in the class folder\n",
    "        for img_name in os.listdir(class_dir):\n",
    "            img_path = class_dir / img_name\n",
    "            \n",
    "            # Read image in grayscale mode\n",
    "            img = cv2.imread(str(img_path), cv2.IMREAD_GRAYSCALE)\n",
    "            \n",
    "            # Skip if image couldn't be read or has wrong dimensions\n",
    "            if img is None or img.shape != (48, 48):\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            # Skip if image is blank or blurry based on heuristics\n",
    "            if is_blank_image(img) or is_blurry(img):\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            # Compute hash of image file to detect duplicates\n",
    "            img_hash = hash_image(img_path)\n",
    "            \n",
    "            # Skip if duplicate image found\n",
    "            if img_hash in seen_hashes:\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            # Mark this hash as seen\n",
    "            seen_hashes.add(img_hash)\n",
    "            \n",
    "            # Preprocess image (normalize pixel values)\n",
    "            processed_img = preprocess_image(img)  # shape: (48, 48)\n",
    "            \n",
    "            # Append image and label to lists\n",
    "            images.append(processed_img)\n",
    "            labels.append(target_class)\n",
    "            saved += 1\n",
    "\n",
    "    print(f\" Finished: {saved} images saved, {skipped} skipped\")\n",
    "    \n",
    "    # Convert lists to numpy arrays before returning\n",
    "    return np.array(images), np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b639b8cc-1ef1-4540-bb01-d4e53f538b7d",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e7b6e2a-77a5-469e-a906-4a407984dea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Processing split: 'train'\n",
      " Finished: 27457 images saved, 1252 skipped\n",
      "\n",
      " Processing split: 'test'\n",
      " Finished: 7089 images saved, 89 skipped\n",
      "\n",
      "🔪 Splitting train into train+val (90.0%/10.0%)\n",
      "\n",
      " Class distribution in train set:\n",
      "  angry: 3804\n",
      "  happy: 6376\n",
      "  neutral: 4379\n",
      "  sad: 4244\n",
      "  stressed: 5908\n",
      "\n",
      " Class distribution in val set:\n",
      "  angry: 423\n",
      "  happy: 709\n",
      "  neutral: 486\n",
      "  sad: 471\n",
      "  stressed: 657\n",
      "\n",
      " Class distribution in test set:\n",
      "  angry: 1054\n",
      "  happy: 1765\n",
      "  neutral: 1225\n",
      "  sad: 1240\n",
      "  stressed: 1805\n",
      "\n",
      " Final Dataset Shapes:\n",
      "Train: X=(24711, 48, 48, 1), y=(24711, 5)\n",
      "Val  : X=(2746, 48, 48, 1), y=(2746, 5)\n",
      "Test : X=(7089, 48, 48, 1), y=(7089, 5)\n",
      "\n",
      "  Class weights (use in training):\n",
      "{0: 1.299211356466877, 1: 0.7751254705144291, 2: 1.1286138387759763, 3: 1.1645146088595664, 4: 0.8365267433987813}\n",
      "\n",
      " Saving .npz files...\n",
      "\n",
      " Preprocessing complete. Files saved in: /app/data/processed/FC211002_Nethmi\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Step 1: Process train and test splits\n",
    "    X_train_full, y_train_full = process_split('train')\n",
    "    X_test, y_test = process_split('test')\n",
    "\n",
    "    # Step 2: Create validation set\n",
    "    print(f\"\\n🔪 Splitting train into train+val ({100*(1-VAL_SPLIT_RATIO)}%/{100*VAL_SPLIT_RATIO}%)\")\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_full, y_train_full,\n",
    "        test_size=VAL_SPLIT_RATIO,\n",
    "        stratify=y_train_full,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Step 3: Label encode + one-hot encode\n",
    "    le = LabelEncoder()\n",
    "    le.fit(y_train_full)\n",
    "\n",
    "    print_class_distribution(y_train, le, \"train\")\n",
    "    print_class_distribution(y_val, le, \"val\")\n",
    "    print_class_distribution(y_test, le, \"test\")\n",
    "\n",
    "    y_train_enc = le.transform(y_train)\n",
    "    y_val_enc = le.transform(y_val)\n",
    "    y_test_enc = le.transform(y_test)\n",
    "\n",
    "    y_train_oh = to_categorical(y_train_enc, num_classes=len(TARGET_CLASSES))\n",
    "    y_val_oh = to_categorical(y_val_enc, num_classes=len(TARGET_CLASSES))\n",
    "    y_test_oh = to_categorical(y_test_enc, num_classes=len(TARGET_CLASSES))\n",
    "\n",
    "    # Step 4: Reshape for model compatibility: (48, 48, 1)\n",
    "    X_train = X_train[..., np.newaxis]\n",
    "    X_val   = X_val[..., np.newaxis]\n",
    "    X_test  = X_test[..., np.newaxis]\n",
    "\n",
    "    # Step 5: Print final shapes\n",
    "    print(\"\\n Final Dataset Shapes:\")\n",
    "    print(f\"Train: X={X_train.shape}, y={y_train_oh.shape}\")\n",
    "    print(f\"Val  : X={X_val.shape}, y={y_val_oh.shape}\")\n",
    "    print(f\"Test : X={X_test.shape}, y={y_test_oh.shape}\")\n",
    "\n",
    "    # Step 6: Compute class weights\n",
    "    class_weights = compute_class_weight(class_weight='balanced',\n",
    "                                         classes=np.unique(y_train_enc),\n",
    "                                         y=y_train_enc)\n",
    "    class_weights_dict = dict(enumerate(class_weights))\n",
    "    print(\"\\n  Class weights (use in training):\")\n",
    "    print(class_weights_dict)\n",
    "\n",
    "    # Step 7: Save to disk\n",
    "    print(\"\\n Saving .npz files...\")\n",
    "    np.savez_compressed(PROCESSED_DIR / \"train.npz\", X=X_train, y=y_train_oh, label_names=le.classes_)\n",
    "    np.savez_compressed(PROCESSED_DIR / \"val.npz\",   X=X_val,   y=y_val_oh,   label_names=le.classes_)\n",
    "    np.savez_compressed(PROCESSED_DIR / \"test.npz\",  X=X_test,  y=y_test_oh,  label_names=le.classes_)\n",
    "\n",
    "    print(\"\\n Preprocessing complete. Files saved in:\", PROCESSED_DIR)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd748fdc-0822-4fc3-a130-c7529f62f376",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
